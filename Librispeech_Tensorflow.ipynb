{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2efccd-8b42-4ff7-a2dc-f5d9cf71fc75",
   "metadata": {},
   "source": [
    "This is my implementation of Jurgen Arias' original Tensorflow model. As far as I can tell, the biggest change is that he had reorganized his LibreSpeech files into male and female directories. He also manually downloaded his Librespeech files, but I am using the torchaudio dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c320fef2-b173-4376-9993-7c3bf565e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "dataroot = os.path.expanduser(\"~\")\n",
    "librispeech_data = torchaudio.datasets.LIBRISPEECH(dataroot, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18098bb2-7f1f-4fae-86b7-1c7613467863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1239/816679470.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "filelist = Path(os.path.join(dataroot, 'LibriSpeech', 'train-clean-100')).rglob('*.flac')\n",
    "\n",
    "files = [{'speaker_id': int(str(file)[len(dataroot)+1:].split('/')[2]), 'file': file} for file in filelist]\n",
    "df_files=pd.DataFrame(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20df8db2-7a92-41fe-9a78-4369e88cae49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>subset</th>\n",
       "      <th>duration</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Kara Shallenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Denny Sayers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>20.139999</td>\n",
       "      <td>Sean McKinley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.010000</td>\n",
       "      <td>Betsie Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Sherry Crowther</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id gender           subset   duration              name\n",
       "3   19      F  train-clean-100  25.190001  Kara Shallenberg\n",
       "8   26      M  train-clean-100  25.080000      Denny Sayers\n",
       "9   27      M  train-clean-100  20.139999     Sean McKinley\n",
       "14  32      F  train-clean-100  24.010000       Betsie Bush\n",
       "18  39      F  train-clean-100  25.049999   Sherry Crowther"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers_file = os.path.join(dataroot, \"LibriSpeech\", \"SPEAKERS.TXT\")\n",
    "speakers = pd.read_table(\n",
    "    speakers_file,\n",
    "    engine='python',\n",
    "    sep='\\s+\\|\\s+',\n",
    "    names=['id','gender','subset','duration','name'],\n",
    "    dtype={'id': 'i','gender': 'U1', 'subset': 'U', 'duration': 'f', 'name': 'U'}, comment=';')\n",
    "speakers = speakers.drop(speakers[\n",
    "    (speakers['subset']=='train-clean-360')\n",
    "    |(speakers['subset']=='train-other-500')\n",
    "    |(speakers['subset']=='dev-other')\n",
    "    |(speakers['subset']=='test-other')\n",
    "].index)\n",
    "speakers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b85482d-72b7-46bd-9df2-db483b74aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although this function was modified and many parameteres were explored with, most of it\n",
    "# came from Source 8 (sources in the READ.ME)\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_features(files):\n",
    "    \n",
    "    # Sets the name to be the path to where the file is in my computer\n",
    "    file_name = str(files.file)\n",
    "\n",
    "    # Loads the audio file as a floating point time series and assigns the default sample rate\n",
    "    # Sample rate is set to 22050 by default\n",
    "    X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "\n",
    "    # Generate Mel-frequency cepstral coefficients (MFCCs) from a time series \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "\n",
    "    # Generates a Short-time Fourier transform (STFT) to use in the chroma_stft\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "\n",
    "    # Computes a chromagram from a waveform or power spectrogram.\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    # Computes a mel-scaled spectrogram.\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    # Computes spectral contrast\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    # Computes the tonal centroid features (tonnetz)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),\n",
    "    sr=sample_rate).T,axis=0)\n",
    "        \n",
    "    \n",
    "    # We add also the classes of each file as a label at the end\n",
    "    label = files.speaker_id\n",
    "\n",
    "    return mfccs, chroma, mel, contrast, tonnetz, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e609e6bd-14d7-4812-83bd-ec98e8663734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features_label from Librispeech_features_label.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28539,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following operation can take hours depending on your cpu.\n",
    "# Luckily, we only have to run it once, then we can save and load the processed data from a pickle file\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "picklefile = 'Librispeech_features_label.pkl'\n",
    "if os.path.isfile(picklefile):\n",
    "    print(f\"Reading features_label from {picklefile}\")\n",
    "    features_label = pd.read_pickle(picklefile)\n",
    "else:\n",
    "    tqdm.pandas()\n",
    "    starttime = datetime.now()\n",
    "    features_label = df_files.progress_apply(extract_features, axis=1)\n",
    "    print(f\"Extracting features took {datetime.now() - startTime}\")\n",
    "features_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff3beb8-67d8-4d93-83a4-7f2ea288b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = []\n",
    "for i in range(0, len(features_label)):\n",
    "    features.append(np.concatenate((features_label[i][0], features_label[i][1], \n",
    "                features_label[i][2], features_label[i][3],\n",
    "                features_label[i][4]), axis=0))\n",
    "df_files['X'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "113fc22d-9ef3-423c-bf0c-f9dd3af70fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 18:58:56.158863: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb = LabelEncoder()\n",
    "labels = to_categorical(lb.fit_transform(df_files['speaker_id']))\n",
    "df_files['y']=labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8d6dc5e-56c8-48c6-887c-1ef76519886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split samples into train/val/test\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "\n",
    "df_files[\"Sets\"] = \"Training\"\n",
    "for lbl in df_files['speaker_id'].unique():\n",
    "    temp_data = df_files[df_files['speaker_id'] == lbl]\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_valid_test_split(\n",
    "        temp_data,\n",
    "        target=\"speaker_id\",\n",
    "        train_size=0.6,\n",
    "        valid_size=0.1,\n",
    "        test_size=0.3\n",
    "    )\n",
    "    df_files.Sets.iloc[X_test.index] = \"Testing\"\n",
    "    df_files.Sets.iloc[X_val.index] = \"Validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc825658-d309-4b05-a7b5-eaee738d01bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf46541f-5bc6-486d-9196-87ff871b0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.fit(np.array(df_files['X'].tolist()))\n",
    "\n",
    "X_train = ss.transform(np.array(df_files[df_files['Sets']=='Training']['X'].tolist()))\n",
    "y_train = np.array(df_files[df_files['Sets']=='Training']['y'].tolist())\n",
    "X_val = ss.transform(np.array(df_files[df_files['Sets']=='Validation']['X'].tolist()))\n",
    "y_val = np.array(df_files[df_files['Sets']=='Validation']['y'].tolist())\n",
    "X_test = ss.transform(np.array(df_files[df_files['Sets']=='Testing']['X'].tolist()))\n",
    "y_test = np.array(df_files[df_files['Sets']=='Testing']['y'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a237ed-9ef9-46c2-83b5-6f7b74031a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 19:04:17.200751: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-04-14 19:04:17.201044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-04-14 19:04:18.749661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-14 19:04:18.750217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 750 Ti computeCapability: 5.0\n",
      "coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\n",
      "2024-04-14 19:04:18.750337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-04-14 19:04:18.803474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-04-14 19:04:18.803720: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-04-14 19:04:18.834009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-04-14 19:04:18.834274: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-04-14 19:04:18.887454: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-04-14 19:04:18.895091: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-04-14 19:04:18.990781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-04-14 19:04:18.991038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-14 19:04:18.991485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-14 19:04:18.991750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2024-04-14 19:04:18.994806: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 19:04:18.996050: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-04-14 19:04:18.996356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-04-14 19:04:18.996747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 750 Ti computeCapability: 5.0\n",
      "coreClock: 1.137GHz coreCount: 5 deviceMemorySize: 1.95GiB deviceMemoryBandwidth: 80.47GiB/s\n",
      "2024-04-14 19:04:18.996850: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-04-14 19:04:18.996956: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-04-14 19:04:18.997000: I tensorflow/stream_executor/platform"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(193, input_shape=(193,), activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(y_train.shape[1], activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b7eab-f0cb-4bbd-8a79-8045d7357eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=256,\n",
    "    epochs=100, \n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "print(f\"{datetime.now()-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f2e4b-1ad5-4f5f-8960-0b3341c5004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_signature = [tf.TensorSpec([1, 193], tf.float32, name='X')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de71c0d-822f-4711-939a-59ce0334da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c7329-d9df-4bad-b3b1-93a8912307a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(onnx_model, 'Librispeech_Tensorflow_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396c1b1-b89f-46a7-8744-63ab72966211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('Librispeech_Tensorflow_X_train.pkl','wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "with open('Librispeech_Tensorflow_y_train.pkl','wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open('Librispeech_Tensorflow_X_val.pkl','wb') as f:\n",
    "    pickle.dump(X_val, f)\n",
    "with open('Librispeech_Tensorflow_y_val.pkl','wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "with open('Librispeech_Tensorflow_X_test.pkl','wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "with open('Librispeech_Tensorflow_y_test.pkl','wb') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9ae86-7ef0-42de-9b18-673d9225527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a732a-bc5e-4ae4-9d3b-3152f2888725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
