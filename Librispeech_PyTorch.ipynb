{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0456a579-1802-4aa7-a0d4-febd1fe9ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "# % pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "#import librosa.display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc70faa5-5f5b-44c5-959f-1e1a1c9998f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_rows=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f03ce2-1dcf-4863-8234-a5426233d41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0016.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0019.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0059.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0011.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0005.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          file\n",
       "0  LibriSpeech/train-clean-100/5393/19218/5393-19218-0016.flac\n",
       "1  LibriSpeech/train-clean-100/5393/19218/5393-19218-0019.flac\n",
       "2  LibriSpeech/train-clean-100/5393/19218/5393-19218-0059.flac\n",
       "3  LibriSpeech/train-clean-100/5393/19218/5393-19218-0011.flac\n",
       "4  LibriSpeech/train-clean-100/5393/19218/5393-19218-0005.flac"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "filelist = Path('LibriSpeech/train-clean-100').rglob('*.flac')\n",
    "\n",
    "df_train = pd.DataFrame(filelist)\n",
    "df_train = df_train.rename(columns={0:'file'})\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f4ba2b-98dc-4615-bcca-7c7c9d3f2f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>subset</th>\n",
       "      <th>duration</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Kara Shallenberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Denny Sayers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>20.139999</td>\n",
       "      <td>Sean McKinley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.010000</td>\n",
       "      <td>Betsie Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Sherry Crowther</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Vicki Barbour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>20.180000</td>\n",
       "      <td>|CBW|Simon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.080000</td>\n",
       "      <td>Paul-Gabriel Wiener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>78</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Hugh McGuire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>83</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Catharine Eastman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>84</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>Christie Nowak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>87</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Rosalind Wills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>89</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Kristen McQuillin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>103</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.719999</td>\n",
       "      <td>Karen Savage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>118</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Alex Buie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>121</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>Nikolle Doolin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>125</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Claire Goget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>150</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Fox in the Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>163</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Andrew Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>174</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>Peter Eastman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>196</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.240000</td>\n",
       "      <td>Stewart Wills</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>198</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.020000</td>\n",
       "      <td>Heather Barnett</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>200</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Maureen S. O'Brien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>201</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Joplin James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>211</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.219999</td>\n",
       "      <td>shanda_w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>226</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.209999</td>\n",
       "      <td>Deb Bacon-Ziegler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>229</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.559999</td>\n",
       "      <td>carnright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>233</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.719999</td>\n",
       "      <td>Steve Karafit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>237</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>rachelellen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>248</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Becky Miller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>250</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Mary Reagan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>251</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>Mark Nelson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>254</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Alan Davis Drake (1945-2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>260</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>Brad Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>289</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.639999</td>\n",
       "      <td>Barbara Wedge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>298</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>Caroline Morse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>302</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Chris Peterson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>307</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>Randy Phillips</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>311</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>deadwhitemales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>322</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>Elisabeth Shields</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>328</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.350000</td>\n",
       "      <td>Elizabeth Palmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>332</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>Aaron Teiser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>374</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>kumarei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>403</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Nocturna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>405</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Eric Dennison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>412</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.379999</td>\n",
       "      <td>Brian Roberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>422</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.380000</td>\n",
       "      <td>President Lethe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>426</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.209999</td>\n",
       "      <td>Norah Piehl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>441</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Sandra in Wales, United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>445</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>11.080000</td>\n",
       "      <td>Dave Foss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>446</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>Steve Hartzog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>458</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>17.299999</td>\n",
       "      <td>Scott Splavec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>460</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>Dave Ranson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>481</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Neal Foley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>587</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>Joy Scaglione</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>625</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>toriasuncle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>652</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.310000</td>\n",
       "      <td>Scott Walter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>669</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Anne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>672</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.270000</td>\n",
       "      <td>Taylor Burton-Edward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>696</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>Tamara R. Schwartz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>730</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Karen Labenz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>777</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>fling93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>831</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.219999</td>\n",
       "      <td>Nick Gallant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>839</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.950001</td>\n",
       "      <td>rovert405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>887</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>Lana Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>908</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>Sam Stinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>909</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>Greg Bryant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>911</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.830000</td>\n",
       "      <td>frankjf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1034</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>16.709999</td>\n",
       "      <td>Kevin O'Coin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1040</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>John Garvin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1069</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>Dawn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>1081</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Fracture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1088</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Christabel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1089</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>Peter Bobbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1098</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>18.309999</td>\n",
       "      <td>Merryb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1116</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Megan Stemm-Wade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1183</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>9.720000</td>\n",
       "      <td>roolynninms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>1188</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>Duncan Murrell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1221</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>Dianne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>1235</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>Tim Gregory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>1246</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>Sandra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>1263</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Leonie Rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>1272</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>John Rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>1284</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>Daniel Anaya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1320</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>number6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1334</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.889999</td>\n",
       "      <td>John Schell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1355</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.309999</td>\n",
       "      <td>Chris Gladis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1363</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.870001</td>\n",
       "      <td>Tammy Sanders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>1447</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Luigina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>1455</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>webslog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>1462</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>E. Tavano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1502</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Ann Boyer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1553</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.490000</td>\n",
       "      <td>Mim Ritty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>1578</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>Lorelle Anderson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1580</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>TinyPines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1594</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Jon Scott Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1624</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.030001</td>\n",
       "      <td>Daniel Shorten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1673</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>Tonia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1723</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Rob Whelan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1737</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Erin Hastings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>1743</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.290001</td>\n",
       "      <td>Bryan Ness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>1841</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Laura Caldwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>1867</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Rowdy Delaney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>1898</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>Jennifer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>1919</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>nprigoda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1926</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Nikki Sullivan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>1963</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Belinda Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1970</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Dawn Larsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1988</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>Ransom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>1992</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>12.290000</td>\n",
       "      <td>Michelle White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1993</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.110000</td>\n",
       "      <td>Wendy Belcher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1995</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>AJai Hilton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>2002</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.940001</td>\n",
       "      <td>Larry Maddocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>2007</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.209999</td>\n",
       "      <td>Sheila Morton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>2035</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.110000</td>\n",
       "      <td>Sharon Bautista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>2078</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.030000</td>\n",
       "      <td>Kathy Caver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>2086</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>Nicodemus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2092</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Elaine Hamby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>2094</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.090000</td>\n",
       "      <td>amycsj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>2136</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Great Plains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>2159</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Matthew Westra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>2182</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>Susan Umpleby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>2196</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.240000</td>\n",
       "      <td>Andrea Fiore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>2277</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>zinniz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>2289</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>David Kleparek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>2300</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.190000</td>\n",
       "      <td>Mitchell L Leopard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>2384</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>20.209999</td>\n",
       "      <td>Ger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>2391</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.540001</td>\n",
       "      <td>treefingers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>2412</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>calystra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>2416</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Julia Albath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>2428</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>Stephen Kinford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>2436</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>Seth Adam Sher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>2514</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>S. Young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>2518</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Rob Powell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>2691</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Donna Stewart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>2764</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Piper Hale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>2803</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>aquielisunari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>2817</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Catherine Millward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>2830</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>Tim Perkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>2836</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Linda McDaniel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>2843</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>ricell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>2893</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.410000</td>\n",
       "      <td>Ryan Sutter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>2902</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>dexter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>2910</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.330000</td>\n",
       "      <td>Janna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>2911</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>David Lawrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>2952</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.020000</td>\n",
       "      <td>Scott Carpenter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>2961</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>Leni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2989</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.209999</td>\n",
       "      <td>Jamie Strassenburg, Cypress, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>3000</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.030000</td>\n",
       "      <td>Brian von Dedenroth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>3081</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>Renata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>3112</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Jessica Louise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>3168</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>David Anton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>3170</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>VOICEGUY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>3214</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>fourteatoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>3235</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.940001</td>\n",
       "      <td>Karen Commins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>3240</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>flakker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>3242</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.030001</td>\n",
       "      <td>peac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>3259</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.190001</td>\n",
       "      <td>Kate West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>3374</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Craig Campbell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>3436</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>Anders Lankford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>3440</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Heidi Will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>3486</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Robin Balmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>3526</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Bereni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>3536</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>Arielle Lipshaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>3570</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>sarac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>3575</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>supergirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>3576</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>JudyGibson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>3607</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>Richard Wallis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>3664</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Barry Eads</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>3699</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>Bruce Pirie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>3723</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.070000</td>\n",
       "      <td>Kevin Lavin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>3729</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.030000</td>\n",
       "      <td>Heather Hogan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>3752</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>Mark Welch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>3807</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.309999</td>\n",
       "      <td>Jesse Noar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>3830</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.010000</td>\n",
       "      <td>rymd80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>3853</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>M. Bertke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>3857</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Epistomolus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>3879</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Keneva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>3947</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.430000</td>\n",
       "      <td>johnell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>3982</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.070000</td>\n",
       "      <td>Kate Adams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>3983</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>lavocedorata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>4014</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>Tom Clifton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>4018</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Nicholas Clifford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>4051</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Liz Devens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>4077</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.140000</td>\n",
       "      <td>Nathan Markham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>4088</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>Blazin48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>4137</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.920000</td>\n",
       "      <td>Sarah LuAnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>4160</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>Rosie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>4195</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>bj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>4214</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>17.450001</td>\n",
       "      <td>A. Janelle Risa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>4267</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Ric F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>4297</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Tina Horning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>4340</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.969999</td>\n",
       "      <td>kiwafruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>4362</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Michelle Montano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>4397</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>John Dennison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>4406</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Matthew Scott Surprenant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>4441</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>William Peck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>4446</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>Jen Maxwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>4481</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.559999</td>\n",
       "      <td>margo zinberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>4507</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>Rachel Nelson-Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>4640</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Karen Mason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>4680</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>pachayes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>4788</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Bill Boerst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>4813</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Steve Mattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>4830</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.049999</td>\n",
       "      <td>George Aalto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>4853</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Barbara Derksen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>4859</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>20.370001</td>\n",
       "      <td>nathank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>4898</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>greatbasinrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>4970</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.150000</td>\n",
       "      <td>airandwaters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>4992</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>Joyce Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>5022</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.120001</td>\n",
       "      <td>Kathleen Costa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>5049</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Bradley Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>5104</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.030001</td>\n",
       "      <td>Chuck Burke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>5105</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.120000</td>\n",
       "      <td>elongman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>5142</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>Mary Ballard-Johansson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>5163</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.570000</td>\n",
       "      <td>LilyAnne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>5192</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.139999</td>\n",
       "      <td>Jason Esteves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>5322</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.030001</td>\n",
       "      <td>Jay Bidal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>5338</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>S R Colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>5339</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Lauren McCullough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>5390</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>Charles Bice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>5393</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.219999</td>\n",
       "      <td>Amy Hengst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>5456</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>e_scarab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>5463</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>GLM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>5514</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>17.389999</td>\n",
       "      <td>Ella Jane Quentin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>5536</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.130000</td>\n",
       "      <td>David Mix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>5561</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.860001</td>\n",
       "      <td>Ellen Jones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>5639</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.280000</td>\n",
       "      <td>Ulf Bjorklund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>5652</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>amicrazy2u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>5678</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>jgoffena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>5683</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>Rachael Lapidis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>5688</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Jennifer Dionne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>5694</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>Winston Tharp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>5703</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Garth Comira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>5750</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>laurencetrask</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>5778</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.809999</td>\n",
       "      <td>Laura Victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>5789</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Kirsten Wever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>5808</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>jeandelfrio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>5867</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.629999</td>\n",
       "      <td>Sharon Omi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>5895</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>iamartin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>6000</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>18.040001</td>\n",
       "      <td>MissRose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>6019</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>DerekP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>6064</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Deborah Knight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>6078</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.020000</td>\n",
       "      <td>dobsonfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>6081</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Lazuli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>6147</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Liberty Stump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>6181</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>Mike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>6209</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>deckerteach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>6241</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>badey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>6272</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>jlenardon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>6295</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>Michael Packard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>6313</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>Jennifer Wiginton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>6319</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>thestorygirl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>6345</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>Jean Bascom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>6367</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Vince Dee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>6385</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>Novella Serena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>6415</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Daryl Wor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>6437</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.090000</td>\n",
       "      <td>John Hoerr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>6454</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.219999</td>\n",
       "      <td>David Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>6476</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Viridian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>6529</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.040001</td>\n",
       "      <td>Fred DeBerardinis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>6531</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.459999</td>\n",
       "      <td>janesandberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>6563</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.580000</td>\n",
       "      <td>William Tomcho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>6818</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>beckyboyd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>6829</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>LadyBug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>6836</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>John</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>6848</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>KarlHenning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>6880</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Capybara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>6925</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>17.070000</td>\n",
       "      <td>Thomas Meaney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>6930</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>Nolan Fout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>7021</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.140000</td>\n",
       "      <td>Nodo420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>7059</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.020000</td>\n",
       "      <td>Joannemmp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>7067</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>Matthew Wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>7078</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Mary in Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>7113</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.200001</td>\n",
       "      <td>Sukaina Jaffer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>7127</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.320000</td>\n",
       "      <td>Bill Kneeland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>7148</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.020000</td>\n",
       "      <td>Vickie Ranz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>7176</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>KalenXI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>7178</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>J.K. Neely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>7190</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.219999</td>\n",
       "      <td>Tony Posante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>7226</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.049999</td>\n",
       "      <td>Jonathan Moore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>7264</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.420000</td>\n",
       "      <td>Sean McClain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>7278</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>Jon Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>7302</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>Asta1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>7312</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>5.440000</td>\n",
       "      <td>nkneer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>7367</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>NIneFive83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>7402</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.010000</td>\n",
       "      <td>Canby Ibarra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>7447</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.240000</td>\n",
       "      <td>dasbury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>7505</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Ron Lockhart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2130</th>\n",
       "      <td>7511</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>Sherri Vance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>7517</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.290001</td>\n",
       "      <td>Raz Mason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>7635</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Judy Guinan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2185</th>\n",
       "      <td>7729</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>Tim Bower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>7780</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>tazzle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>7794</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>mlcui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>7800</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.209999</td>\n",
       "      <td>Arie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>7850</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>Jill Engle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>7859</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>21.580000</td>\n",
       "      <td>xinamarieuhl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>7976</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.130000</td>\n",
       "      <td>JenniferRutters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>8014</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>14.420000</td>\n",
       "      <td>constatine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>8051</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>Maria Kasper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>8063</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Robert Snoza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2289</th>\n",
       "      <td>8088</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Jason Bolestridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>8095</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.030001</td>\n",
       "      <td>Theodulf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>8098</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Arnold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>8108</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.030001</td>\n",
       "      <td>drakaunus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>8123</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.150000</td>\n",
       "      <td>Sheila Wood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>8224</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>Leanne Kinkopf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>8226</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>Adam Picot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>8230</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>David Jenkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>8238</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.080000</td>\n",
       "      <td>Madam Fickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2348</th>\n",
       "      <td>8297</td>\n",
       "      <td>M</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.040000</td>\n",
       "      <td>David Mecionis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>8312</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.139999</td>\n",
       "      <td>Jaimie Noy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>8324</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Kathy Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>8419</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.219999</td>\n",
       "      <td>Jon Kissack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>8425</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.160000</td>\n",
       "      <td>Larry Wilson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>8455</td>\n",
       "      <td>M</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.030000</td>\n",
       "      <td>thecheops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>8463</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.050000</td>\n",
       "      <td>Michele Fry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>8465</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>24.879999</td>\n",
       "      <td>TinaNygard2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>8468</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>Jennifer Dorr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2414</th>\n",
       "      <td>8555</td>\n",
       "      <td>F</td>\n",
       "      <td>test-clean</td>\n",
       "      <td>8.030000</td>\n",
       "      <td>Michelle Goode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>8580</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>19.820000</td>\n",
       "      <td>Gary Dana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>8609</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>noblesavage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>8629</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Shivansh Dhar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>8630</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>Eduardo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>8747</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>23.490000</td>\n",
       "      <td>DeanOBuchanan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>8770</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.129999</td>\n",
       "      <td>Paul Simonin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>8797</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>22.760000</td>\n",
       "      <td>Sean Grabosky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>8838</td>\n",
       "      <td>M</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.059999</td>\n",
       "      <td>Kevin Owens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>8842</td>\n",
       "      <td>F</td>\n",
       "      <td>dev-clean</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>Mary J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>8975</td>\n",
       "      <td>F</td>\n",
       "      <td>train-clean-100</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>Daisy Flaim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender           subset   duration  \\\n",
       "3       19      F  train-clean-100  25.190001   \n",
       "8       26      M  train-clean-100  25.080000   \n",
       "9       27      M  train-clean-100  20.139999   \n",
       "14      32      F  train-clean-100  24.010000   \n",
       "18      39      F  train-clean-100  25.049999   \n",
       "19      40      F  train-clean-100  25.040001   \n",
       "32      60      M  train-clean-100  20.180000   \n",
       "33      61      M       test-clean   8.080000   \n",
       "41      78      M  train-clean-100  25.049999   \n",
       "45      83      F  train-clean-100  25.040001   \n",
       "46      84      F        dev-clean   8.020000   \n",
       "48      87      F  train-clean-100  25.100000   \n",
       "49      89      F  train-clean-100  25.080000   \n",
       "59     103      F  train-clean-100  23.719999   \n",
       "68     118      M  train-clean-100  25.049999   \n",
       "70     121      F       test-clean   8.010000   \n",
       "73     125      F  train-clean-100  25.180000   \n",
       "80     150      F  train-clean-100  25.129999   \n",
       "88     163      M  train-clean-100  25.059999   \n",
       "93     174      M        dev-clean   8.040000   \n",
       "99     196      M  train-clean-100  25.240000   \n",
       "100    198      F  train-clean-100  25.020000   \n",
       "102    200      F  train-clean-100  25.139999   \n",
       "103    201      M  train-clean-100  25.190001   \n",
       "112    211      F  train-clean-100  25.219999   \n",
       "119    226      F  train-clean-100  25.209999   \n",
       "122    229      M  train-clean-100  19.559999   \n",
       "124    233      M  train-clean-100  24.719999   \n",
       "125    237      F       test-clean   8.020000   \n",
       "132    248      F  train-clean-100  25.129999   \n",
       "134    250      F  train-clean-100  25.160000   \n",
       "135    251      M        dev-clean   8.040000   \n",
       "138    254      M  train-clean-100  25.059999   \n",
       "141    260      M       test-clean   8.050000   \n",
       "152    289      F  train-clean-100  19.639999   \n",
       "155    298      F  train-clean-100  25.000000   \n",
       "156    302      F  train-clean-100  25.040001   \n",
       "158    307      M  train-clean-100  25.110001   \n",
       "160    311      M  train-clean-100  25.170000   \n",
       "165    322      F  train-clean-100  25.200001   \n",
       "167    328      F  train-clean-100  19.350000   \n",
       "170    332      M  train-clean-100  19.000000   \n",
       "186    374      M  train-clean-100  25.160000   \n",
       "192    403      F  train-clean-100  25.139999   \n",
       "194    405      M  train-clean-100  25.040001   \n",
       "197    412      M  train-clean-100  22.379999   \n",
       "200    422      M        dev-clean   8.380000   \n",
       "201    426      F  train-clean-100  25.209999   \n",
       "206    441      F  train-clean-100  25.190001   \n",
       "208    445      M  train-clean-100  11.080000   \n",
       "209    446      M  train-clean-100  25.000000   \n",
       "214    458      M  train-clean-100  17.299999   \n",
       "216    460      M  train-clean-100  25.230000   \n",
       "226    481      M  train-clean-100  25.080000   \n",
       "261    587      F  train-clean-100  25.200001   \n",
       "274    625      M  train-clean-100  25.010000   \n",
       "279    652      M        dev-clean   8.310000   \n",
       "284    669      F  train-clean-100  25.170000   \n",
       "286    672      M       test-clean   8.270000   \n",
       "292    696      F  train-clean-100  25.110001   \n",
       "309    730      F  train-clean-100  25.190001   \n",
       "318    777      M        dev-clean   8.060000   \n",
       "340    831      M  train-clean-100  25.219999   \n",
       "344    839      M  train-clean-100  19.950001   \n",
       "355    887      F  train-clean-100  25.110001   \n",
       "358    908      M       test-clean   8.050000   \n",
       "359    909      M  train-clean-100  25.000000   \n",
       "360    911      M  train-clean-100  23.830000   \n",
       "395   1034      M  train-clean-100  16.709999   \n",
       "396   1040      M  train-clean-100  15.700000   \n",
       "409   1069      F  train-clean-100  25.150000   \n",
       "411   1081      M  train-clean-100  25.180000   \n",
       "414   1088      F  train-clean-100  25.180000   \n",
       "415   1089      M       test-clean   8.050000   \n",
       "421   1098      F  train-clean-100  18.309999   \n",
       "426   1116      F  train-clean-100  25.120001   \n",
       "441   1183      F  train-clean-100   9.720000   \n",
       "444   1188      M       test-clean   8.200000   \n",
       "448   1221      F       test-clean   8.070000   \n",
       "454   1235      M  train-clean-100  25.090000   \n",
       "457   1246      F  train-clean-100  25.090000   \n",
       "465   1263      F  train-clean-100  25.160000   \n",
       "470   1272      M        dev-clean   8.020000   \n",
       "474   1284      F       test-clean   8.160000   \n",
       "483   1320      M       test-clean   8.020000   \n",
       "487   1334      M  train-clean-100  23.889999   \n",
       "498   1355      M  train-clean-100  22.309999   \n",
       "499   1363      F  train-clean-100  21.870001   \n",
       "524   1447      F  train-clean-100  25.180000   \n",
       "526   1455      M  train-clean-100  25.129999   \n",
       "529   1462      F        dev-clean   8.040000   \n",
       "542   1502      F  train-clean-100  25.049999   \n",
       "552   1553      F  train-clean-100  22.490000   \n",
       "561   1578      F  train-clean-100  25.010000   \n",
       "563   1580      F       test-clean   8.070000   \n",
       "566   1594      M  train-clean-100  25.100000   \n",
       "574   1624      M  train-clean-100  22.030001   \n",
       "595   1673      F        dev-clean   8.070000   \n",
       "619   1723      M  train-clean-100  25.120001   \n",
       "626   1737      F  train-clean-100  25.040001   \n",
       "628   1743      M  train-clean-100  21.290001   \n",
       "662   1841      F  train-clean-100  25.129999   \n",
       "670   1867      M  train-clean-100  25.160000   \n",
       "676   1898      F  train-clean-100  25.010000   \n",
       "681   1919      F        dev-clean   8.170000   \n",
       "685   1926      F  train-clean-100  25.190001   \n",
       "693   1963      F  train-clean-100  25.190001   \n",
       "695   1970      F  train-clean-100  25.129999   \n",
       "700   1988      F        dev-clean   8.160000   \n",
       "702   1992      F  train-clean-100  12.290000   \n",
       "703   1993      F        dev-clean   8.110000   \n",
       "704   1995      F       test-clean   8.060000   \n",
       "707   2002      M  train-clean-100  24.940001   \n",
       "710   2007      F  train-clean-100  25.209999   \n",
       "717   2035      F        dev-clean   8.110000   \n",
       "733   2078      M        dev-clean   8.030000   \n",
       "735   2086      M        dev-clean   8.040000   \n",
       "738   2092      F  train-clean-100  25.100000   \n",
       "740   2094      F       test-clean   8.090000   \n",
       "749   2136      M  train-clean-100  25.180000   \n",
       "758   2159      M  train-clean-100  25.120001   \n",
       "761   2182      F  train-clean-100  25.150000   \n",
       "765   2196      F  train-clean-100  25.240000   \n",
       "786   2277      F        dev-clean   8.010000   \n",
       "791   2289      M  train-clean-100  25.080000   \n",
       "796   2300      M       test-clean   8.190000   \n",
       "813   2384      M  train-clean-100  20.209999   \n",
       "815   2391      F  train-clean-100  24.540001   \n",
       "823   2412      F        dev-clean   8.060000   \n",
       "825   2416      F  train-clean-100  25.170000   \n",
       "827   2428      M        dev-clean   8.020000   \n",
       "828   2436      M  train-clean-100  25.150000   \n",
       "845   2514      M  train-clean-100  25.110001   \n",
       "847   2518      M  train-clean-100  25.059999   \n",
       "887   2691      F  train-clean-100  25.160000   \n",
       "903   2764      F  train-clean-100  25.190001   \n",
       "911   2803      M        dev-clean   8.200000   \n",
       "915   2817      F  train-clean-100  25.139999   \n",
       "919   2830      M       test-clean   8.040000   \n",
       "921   2836      F  train-clean-100  25.120001   \n",
       "922   2843      M  train-clean-100  25.180000   \n",
       "926   2893      M  train-clean-100  24.410000   \n",
       "928   2902      M        dev-clean   8.100000   \n",
       "930   2910      F  train-clean-100  22.330000   \n",
       "931   2911      M  train-clean-100  25.100000   \n",
       "939   2952      M  train-clean-100  25.020000   \n",
       "941   2961      F       test-clean   8.070000   \n",
       "948   2989      F  train-clean-100  25.209999   \n",
       "954   3000      M        dev-clean   8.030000   \n",
       "976   3081      F        dev-clean   8.000000   \n",
       "988   3112      F  train-clean-100  25.080000   \n",
       "1002  3168      M  train-clean-100  23.900000   \n",
       "1003  3170      M        dev-clean   8.100000   \n",
       "1012  3214      M  train-clean-100  25.080000   \n",
       "1019  3235      F  train-clean-100  24.940001   \n",
       "1021  3240      M  train-clean-100  25.090000   \n",
       "1022  3242      M  train-clean-100  25.030001   \n",
       "1027  3259      F  train-clean-100  25.190001   \n",
       "1055  3374      M  train-clean-100  25.170000   \n",
       "1066  3436      M  train-clean-100  25.250000   \n",
       "1067  3440      F  train-clean-100  25.180000   \n",
       "1076  3486      M  train-clean-100  25.080000   \n",
       "1084  3526      F  train-clean-100  25.180000   \n",
       "1086  3536      F        dev-clean   8.150000   \n",
       "1101  3570      F       test-clean   8.050000   \n",
       "1103  3575      F       test-clean   8.060000   \n",
       "1104  3576      F        dev-clean   8.000000   \n",
       "1112  3607      M  train-clean-100  22.100000   \n",
       "1126  3664      M  train-clean-100  25.059999   \n",
       "1134  3699      M  train-clean-100  25.150000   \n",
       "1137  3723      M  train-clean-100  25.070000   \n",
       "1139  3729      F       test-clean   8.030000   \n",
       "1144  3752      M        dev-clean   8.060000   \n",
       "1156  3807      M  train-clean-100  22.309999   \n",
       "1160  3830      M  train-clean-100  24.010000   \n",
       "1167  3853      F        dev-clean   8.050000   \n",
       "1168  3857      M  train-clean-100  25.170000   \n",
       "1175  3879      F  train-clean-100  25.100000   \n",
       "1196  3947      F  train-clean-100  22.430000   \n",
       "1205  3982      F  train-clean-100  25.070000   \n",
       "1206  3983      F  train-clean-100  25.010000   \n",
       "1216  4014      M  train-clean-100  25.200001   \n",
       "1219  4018      M  train-clean-100  25.170000   \n",
       "1227  4051      F  train-clean-100  25.139999   \n",
       "1234  4077      M       test-clean   8.140000   \n",
       "1237  4088      F  train-clean-100  25.150000   \n",
       "1247  4137      F  train-clean-100  24.920000   \n",
       "1254  4160      F  train-clean-100  25.090000   \n",
       "1263  4195      F  train-clean-100  25.080000   \n",
       "1268  4214      F  train-clean-100  17.450001   \n",
       "1285  4267      M  train-clean-100  25.139999   \n",
       "1294  4297      F  train-clean-100  25.040001   \n",
       "1303  4340      F  train-clean-100  21.969999   \n",
       "1310  4362      F  train-clean-100  25.180000   \n",
       "1315  4397      M  train-clean-100  25.059999   \n",
       "1317  4406      M  train-clean-100  25.120001   \n",
       "1330  4441      M  train-clean-100  25.059999   \n",
       "1333  4446      F       test-clean   8.000000   \n",
       "1339  4481      F  train-clean-100  22.559999   \n",
       "1345  4507      F       test-clean   8.050000   \n",
       "1369  4640      F  train-clean-100  25.059999   \n",
       "1374  4680      F  train-clean-100  25.150000   \n",
       "1403  4788      M  train-clean-100  25.040001   \n",
       "1409  4813      M  train-clean-100  25.120001   \n",
       "1412  4830      M  train-clean-100  24.049999   \n",
       "1421  4853      F  train-clean-100  25.040001   \n",
       "1424  4859      M  train-clean-100  20.370001   \n",
       "1429  4898      M  train-clean-100  25.110001   \n",
       "1447  4970      F       test-clean   8.150000   \n",
       "1451  4992      F       test-clean   8.210000   \n",
       "1461  5022      F  train-clean-100  25.120001   \n",
       "1470  5049      M  train-clean-100  25.080000   \n",
       "1481  5104      M  train-clean-100  25.030001   \n",
       "1482  5105      M       test-clean   8.120000   \n",
       "1492  5142      F       test-clean   8.070000   \n",
       "1497  5163      F  train-clean-100  23.570000   \n",
       "1506  5192      M  train-clean-100  24.139999   \n",
       "1537  5322      M  train-clean-100  25.030001   \n",
       "1542  5338      F        dev-clean   8.070000   \n",
       "1543  5339      F  train-clean-100  25.129999   \n",
       "1552  5390      M  train-clean-100  25.200001   \n",
       "1553  5393      F  train-clean-100  25.219999   \n",
       "1564  5456      M  train-clean-100  19.770000   \n",
       "1567  5463      M  train-clean-100  25.000000   \n",
       "1576  5514      F  train-clean-100  17.389999   \n",
       "1578  5536      M        dev-clean   8.130000   \n",
       "1582  5561      F  train-clean-100  23.860001   \n",
       "1598  5639      M       test-clean   8.280000   \n",
       "1601  5652      F  train-clean-100  25.049999   \n",
       "1610  5678      M  train-clean-100  25.100000   \n",
       "1612  5683      F       test-clean   8.010000   \n",
       "1614  5688      F  train-clean-100  25.170000   \n",
       "1615  5694      M        dev-clean   8.010000   \n",
       "1617  5703      M  train-clean-100  25.100000   \n",
       "1631  5750      M  train-clean-100  25.080000   \n",
       "1638  5778      F  train-clean-100  21.809999   \n",
       "1641  5789      F  train-clean-100  25.160000   \n",
       "1645  5808      M  train-clean-100  25.160000   \n",
       "1656  5867      F  train-clean-100  23.629999   \n",
       "1665  5895      F        dev-clean   8.020000   \n",
       "1690  6000      F  train-clean-100  18.040001   \n",
       "1696  6019      M  train-clean-100  25.170000   \n",
       "1706  6064      F  train-clean-100  25.040001   \n",
       "1713  6078      F  train-clean-100  25.020000   \n",
       "1715  6081      M  train-clean-100  25.100000   \n",
       "1740  6147      F  train-clean-100  25.049999   \n",
       "1749  6181      M  train-clean-100  25.090000   \n",
       "1756  6209      M  train-clean-100  25.040001   \n",
       "1766  6241      M        dev-clean   8.050000   \n",
       "1775  6272      F  train-clean-100  25.139999   \n",
       "1782  6295      M        dev-clean   8.040000   \n",
       "1786  6313      F        dev-clean   8.170000   \n",
       "1788  6319      F        dev-clean   8.010000   \n",
       "1796  6345      F        dev-clean   8.070000   \n",
       "1805  6367      M  train-clean-100  25.040001   \n",
       "1812  6385      F  train-clean-100  25.090000   \n",
       "1821  6415      F  train-clean-100  25.180000   \n",
       "1826  6437      M  train-clean-100  25.090000   \n",
       "1828  6454      M  train-clean-100  25.219999   \n",
       "1833  6476      F  train-clean-100  25.170000   \n",
       "1849  6529      M  train-clean-100  25.040001   \n",
       "1850  6531      F  train-clean-100  21.459999   \n",
       "1864  6563      M  train-clean-100  21.580000   \n",
       "1929  6818      F  train-clean-100  25.170000   \n",
       "1932  6829      F       test-clean   8.240000   \n",
       "1933  6836      M  train-clean-100  25.010000   \n",
       "1936  6848      M  train-clean-100  25.170000   \n",
       "1942  6880      M  train-clean-100  25.100000   \n",
       "1956  6925      M  train-clean-100  17.070000   \n",
       "1958  6930      M       test-clean   8.000000   \n",
       "1983  7021      M       test-clean   8.140000   \n",
       "1989  7059      F  train-clean-100  25.020000   \n",
       "1993  7067      M  train-clean-100  25.170000   \n",
       "1996  7078      F  train-clean-100  25.139999   \n",
       "2006  7113      F  train-clean-100  25.200001   \n",
       "2012  7127      M       test-clean   8.320000   \n",
       "2023  7148      F  train-clean-100  25.020000   \n",
       "2028  7176      M       test-clean   8.060000   \n",
       "2030  7178      F  train-clean-100  25.110001   \n",
       "2033  7190      M  train-clean-100  25.219999   \n",
       "2043  7226      M  train-clean-100  25.049999   \n",
       "2058  7264      M  train-clean-100  23.420000   \n",
       "2062  7278      M  train-clean-100  25.010000   \n",
       "2069  7302      F  train-clean-100  25.100000   \n",
       "2071  7312      M  train-clean-100   5.440000   \n",
       "2092  7367      M  train-clean-100  25.129999   \n",
       "2102  7402      M  train-clean-100  25.010000   \n",
       "2112  7447      M  train-clean-100  25.240000   \n",
       "2127  7505      M  train-clean-100  25.129999   \n",
       "2130  7511      F  train-clean-100  25.000000   \n",
       "2134  7517      F  train-clean-100  24.290001   \n",
       "2160  7635      F  train-clean-100  25.059999   \n",
       "2185  7729      M       test-clean   8.170000   \n",
       "2201  7780      F  train-clean-100  23.500000   \n",
       "2205  7794      F  train-clean-100  25.230000   \n",
       "2208  7800      F  train-clean-100  25.209999   \n",
       "2223  7850      F        dev-clean   8.060000   \n",
       "2224  7859      F  train-clean-100  21.580000   \n",
       "2256  7976      M        dev-clean   8.130000   \n",
       "2270  8014      F  train-clean-100  14.420000   \n",
       "2279  8051      F  train-clean-100  25.230000   \n",
       "2282  8063      M  train-clean-100  25.080000   \n",
       "2289  8088      M  train-clean-100  25.059999   \n",
       "2290  8095      M  train-clean-100  25.030001   \n",
       "2292  8098      M  train-clean-100  25.180000   \n",
       "2293  8108      M  train-clean-100  25.030001   \n",
       "2298  8123      F  train-clean-100  25.150000   \n",
       "2326  8224      M       test-clean   8.240000   \n",
       "2328  8226      M  train-clean-100  25.180000   \n",
       "2330  8230      M       test-clean   8.250000   \n",
       "2331  8238      F  train-clean-100  25.080000   \n",
       "2348  8297      M        dev-clean   8.040000   \n",
       "2352  8312      F  train-clean-100  25.139999   \n",
       "2356  8324      F  train-clean-100  25.160000   \n",
       "2377  8419      M  train-clean-100  25.219999   \n",
       "2381  8425      M  train-clean-100  25.160000   \n",
       "2389  8455      M       test-clean   8.030000   \n",
       "2392  8463      F       test-clean   8.050000   \n",
       "2394  8465      F  train-clean-100  24.879999   \n",
       "2396  8468      F  train-clean-100  25.230000   \n",
       "2414  8555      F       test-clean   8.030000   \n",
       "2419  8580      M  train-clean-100  19.820000   \n",
       "2425  8609      M  train-clean-100  25.100000   \n",
       "2428  8629      M  train-clean-100  25.129999   \n",
       "2429  8630      M  train-clean-100  23.500000   \n",
       "2451  8747      M  train-clean-100  23.490000   \n",
       "2455  8770      M  train-clean-100  25.129999   \n",
       "2462  8797      M  train-clean-100  22.760000   \n",
       "2469  8838      M  train-clean-100  25.059999   \n",
       "2470  8842      F        dev-clean   8.100000   \n",
       "2479  8975      F  train-clean-100  25.110001   \n",
       "\n",
       "                                         name  \n",
       "3                            Kara Shallenberg  \n",
       "8                                Denny Sayers  \n",
       "9                               Sean McKinley  \n",
       "14                                Betsie Bush  \n",
       "18                            Sherry Crowther  \n",
       "19                              Vicki Barbour  \n",
       "32                                 |CBW|Simon  \n",
       "33                        Paul-Gabriel Wiener  \n",
       "41                               Hugh McGuire  \n",
       "45                          Catharine Eastman  \n",
       "46                             Christie Nowak  \n",
       "48                             Rosalind Wills  \n",
       "49                          Kristen McQuillin  \n",
       "59                               Karen Savage  \n",
       "68                                  Alex Buie  \n",
       "70                             Nikolle Doolin  \n",
       "73                               Claire Goget  \n",
       "80                           Fox in the Stars  \n",
       "88                              Andrew Miller  \n",
       "93                              Peter Eastman  \n",
       "99                              Stewart Wills  \n",
       "100                           Heather Barnett  \n",
       "102                        Maureen S. O'Brien  \n",
       "103                              Joplin James  \n",
       "112                                  shanda_w  \n",
       "119                         Deb Bacon-Ziegler  \n",
       "122                                 carnright  \n",
       "124                             Steve Karafit  \n",
       "125                               rachelellen  \n",
       "132                              Becky Miller  \n",
       "134                               Mary Reagan  \n",
       "135                               Mark Nelson  \n",
       "138              Alan Davis Drake (1945-2010)  \n",
       "141                                 Brad Bush  \n",
       "152                             Barbara Wedge  \n",
       "155                            Caroline Morse  \n",
       "156                            Chris Peterson  \n",
       "158                            Randy Phillips  \n",
       "160                            deadwhitemales  \n",
       "165                         Elisabeth Shields  \n",
       "167                          Elizabeth Palmer  \n",
       "170                              Aaron Teiser  \n",
       "186                                   kumarei  \n",
       "192                                  Nocturna  \n",
       "194                             Eric Dennison  \n",
       "197                              Brian Roberg  \n",
       "200                           President Lethe  \n",
       "201                               Norah Piehl  \n",
       "206           Sandra in Wales, United Kingdom  \n",
       "208                                 Dave Foss  \n",
       "209                             Steve Hartzog  \n",
       "214                             Scott Splavec  \n",
       "216                               Dave Ranson  \n",
       "226                                Neal Foley  \n",
       "261                             Joy Scaglione  \n",
       "274                               toriasuncle  \n",
       "279                              Scott Walter  \n",
       "284                                      Anne  \n",
       "286                      Taylor Burton-Edward  \n",
       "292                        Tamara R. Schwartz  \n",
       "309                              Karen Labenz  \n",
       "318                                   fling93  \n",
       "340                              Nick Gallant  \n",
       "344                                 rovert405  \n",
       "355                               Lana Taylor  \n",
       "358                               Sam Stinson  \n",
       "359                               Greg Bryant  \n",
       "360                                   frankjf  \n",
       "395                              Kevin O'Coin  \n",
       "396                               John Garvin  \n",
       "409                                      Dawn  \n",
       "411                                  Fracture  \n",
       "414                                Christabel  \n",
       "415                               Peter Bobbe  \n",
       "421                                    Merryb  \n",
       "426                          Megan Stemm-Wade  \n",
       "441                               roolynninms  \n",
       "444                            Duncan Murrell  \n",
       "448                                    Dianne  \n",
       "454                               Tim Gregory  \n",
       "457                                    Sandra  \n",
       "465                               Leonie Rose  \n",
       "470                                 John Rose  \n",
       "474                              Daniel Anaya  \n",
       "483                                   number6  \n",
       "487                               John Schell  \n",
       "498                              Chris Gladis  \n",
       "499                             Tammy Sanders  \n",
       "524                                   Luigina  \n",
       "526                                   webslog  \n",
       "529                                 E. Tavano  \n",
       "542                                 Ann Boyer  \n",
       "552                                 Mim Ritty  \n",
       "561                          Lorelle Anderson  \n",
       "563                                 TinyPines  \n",
       "566                           Jon Scott Jones  \n",
       "574                            Daniel Shorten  \n",
       "595                                     Tonia  \n",
       "619                                Rob Whelan  \n",
       "626                             Erin Hastings  \n",
       "628                                Bryan Ness  \n",
       "662                            Laura Caldwell  \n",
       "670                             Rowdy Delaney  \n",
       "676                                  Jennifer  \n",
       "681                                  nprigoda  \n",
       "685                            Nikki Sullivan  \n",
       "693                             Belinda Brown  \n",
       "695                               Dawn Larsen  \n",
       "700                                    Ransom  \n",
       "702                            Michelle White  \n",
       "703                             Wendy Belcher  \n",
       "704                               AJai Hilton  \n",
       "707                            Larry Maddocks  \n",
       "710                             Sheila Morton  \n",
       "717                           Sharon Bautista  \n",
       "733                               Kathy Caver  \n",
       "735                                 Nicodemus  \n",
       "738                              Elaine Hamby  \n",
       "740                                    amycsj  \n",
       "749                              Great Plains  \n",
       "758                            Matthew Westra  \n",
       "761                             Susan Umpleby  \n",
       "765                              Andrea Fiore  \n",
       "786                                    zinniz  \n",
       "791                            David Kleparek  \n",
       "796                        Mitchell L Leopard  \n",
       "813                                       Ger  \n",
       "815                               treefingers  \n",
       "823                                  calystra  \n",
       "825                              Julia Albath  \n",
       "827                           Stephen Kinford  \n",
       "828                            Seth Adam Sher  \n",
       "845                                  S. Young  \n",
       "847                                Rob Powell  \n",
       "887                             Donna Stewart  \n",
       "903                                Piper Hale  \n",
       "911                             aquielisunari  \n",
       "915                        Catherine Millward  \n",
       "919                               Tim Perkins  \n",
       "921                            Linda McDaniel  \n",
       "922                                    ricell  \n",
       "926                               Ryan Sutter  \n",
       "928                                    dexter  \n",
       "930                                     Janna  \n",
       "931                            David Lawrence  \n",
       "939                           Scott Carpenter  \n",
       "941                                      Leni  \n",
       "948   Jamie Strassenburg, Cypress, California  \n",
       "954                       Brian von Dedenroth  \n",
       "976                                    Renata  \n",
       "988                            Jessica Louise  \n",
       "1002                              David Anton  \n",
       "1003                                 VOICEGUY  \n",
       "1012                               fourteatoo  \n",
       "1019                            Karen Commins  \n",
       "1021                                  flakker  \n",
       "1022                                     peac  \n",
       "1027                                Kate West  \n",
       "1055                           Craig Campbell  \n",
       "1066                          Anders Lankford  \n",
       "1067                               Heidi Will  \n",
       "1076                             Robin Balmer  \n",
       "1084                                   Bereni  \n",
       "1086                          Arielle Lipshaw  \n",
       "1101                                    sarac  \n",
       "1103                                supergirl  \n",
       "1104                               JudyGibson  \n",
       "1112                           Richard Wallis  \n",
       "1126                               Barry Eads  \n",
       "1134                              Bruce Pirie  \n",
       "1137                              Kevin Lavin  \n",
       "1139                            Heather Hogan  \n",
       "1144                               Mark Welch  \n",
       "1156                               Jesse Noar  \n",
       "1160                                   rymd80  \n",
       "1167                                M. Bertke  \n",
       "1168                              Epistomolus  \n",
       "1175                                   Keneva  \n",
       "1196                                  johnell  \n",
       "1205                               Kate Adams  \n",
       "1206                             lavocedorata  \n",
       "1216                              Tom Clifton  \n",
       "1219                        Nicholas Clifford  \n",
       "1227                               Liz Devens  \n",
       "1234                           Nathan Markham  \n",
       "1237                                 Blazin48  \n",
       "1247                              Sarah LuAnn  \n",
       "1254                                    Rosie  \n",
       "1263                                       bj  \n",
       "1268                          A. Janelle Risa  \n",
       "1285                                    Ric F  \n",
       "1294                             Tina Horning  \n",
       "1303                                kiwafruit  \n",
       "1310                         Michelle Montano  \n",
       "1315                            John Dennison  \n",
       "1317                 Matthew Scott Surprenant  \n",
       "1330                             William Peck  \n",
       "1333                              Jen Maxwell  \n",
       "1339                            margo zinberg  \n",
       "1345                      Rachel Nelson-Smith  \n",
       "1369                              Karen Mason  \n",
       "1374                                 pachayes  \n",
       "1403                              Bill Boerst  \n",
       "1409                            Steve Mattern  \n",
       "1412                             George Aalto  \n",
       "1421                          Barbara Derksen  \n",
       "1424                                  nathank  \n",
       "1429                           greatbasinrain  \n",
       "1447                             airandwaters  \n",
       "1451                             Joyce Martin  \n",
       "1461                           Kathleen Costa  \n",
       "1470                            Bradley Smith  \n",
       "1481                              Chuck Burke  \n",
       "1482                                 elongman  \n",
       "1492                   Mary Ballard-Johansson  \n",
       "1497                                 LilyAnne  \n",
       "1506                            Jason Esteves  \n",
       "1537                                Jay Bidal  \n",
       "1542                                S R Colon  \n",
       "1543                        Lauren McCullough  \n",
       "1552                             Charles Bice  \n",
       "1553                               Amy Hengst  \n",
       "1564                                 e_scarab  \n",
       "1567                                      GLM  \n",
       "1576                        Ella Jane Quentin  \n",
       "1578                                David Mix  \n",
       "1582                              Ellen Jones  \n",
       "1598                            Ulf Bjorklund  \n",
       "1601                               amicrazy2u  \n",
       "1610                                 jgoffena  \n",
       "1612                          Rachael Lapidis  \n",
       "1614                          Jennifer Dionne  \n",
       "1615                            Winston Tharp  \n",
       "1617                             Garth Comira  \n",
       "1631                            laurencetrask  \n",
       "1638                           Laura Victoria  \n",
       "1641                            Kirsten Wever  \n",
       "1645                              jeandelfrio  \n",
       "1656                               Sharon Omi  \n",
       "1665                                 iamartin  \n",
       "1690                                 MissRose  \n",
       "1696                                   DerekP  \n",
       "1706                           Deborah Knight  \n",
       "1713                                dobsonfly  \n",
       "1715                                   Lazuli  \n",
       "1740                            Liberty Stump  \n",
       "1749                                     Mike  \n",
       "1756                              deckerteach  \n",
       "1766                                    badey  \n",
       "1775                                jlenardon  \n",
       "1782                          Michael Packard  \n",
       "1786                        Jennifer Wiginton  \n",
       "1788                             thestorygirl  \n",
       "1796                              Jean Bascom  \n",
       "1805                                Vince Dee  \n",
       "1812                           Novella Serena  \n",
       "1821                                Daryl Wor  \n",
       "1826                               John Hoerr  \n",
       "1828                              David Wales  \n",
       "1833                                 Viridian  \n",
       "1849                        Fred DeBerardinis  \n",
       "1850                             janesandberg  \n",
       "1864                           William Tomcho  \n",
       "1929                                beckyboyd  \n",
       "1932                                  LadyBug  \n",
       "1933                                     John  \n",
       "1936                              KarlHenning  \n",
       "1942                                 Capybara  \n",
       "1956                            Thomas Meaney  \n",
       "1958                               Nolan Fout  \n",
       "1983                                  Nodo420  \n",
       "1989                                Joannemmp  \n",
       "1993                             Matthew Wall  \n",
       "1996                         Mary in Arkansas  \n",
       "2006                           Sukaina Jaffer  \n",
       "2012                            Bill Kneeland  \n",
       "2023                              Vickie Ranz  \n",
       "2028                                  KalenXI  \n",
       "2030                               J.K. Neely  \n",
       "2033                             Tony Posante  \n",
       "2043                           Jonathan Moore  \n",
       "2058                             Sean McClain  \n",
       "2062                                Jon Smith  \n",
       "2069                                 Asta1234  \n",
       "2071                                   nkneer  \n",
       "2092                               NIneFive83  \n",
       "2102                             Canby Ibarra  \n",
       "2112                                  dasbury  \n",
       "2127                             Ron Lockhart  \n",
       "2130                             Sherri Vance  \n",
       "2134                                Raz Mason  \n",
       "2160                              Judy Guinan  \n",
       "2185                                Tim Bower  \n",
       "2201                                   tazzle  \n",
       "2205                                    mlcui  \n",
       "2208                                     Arie  \n",
       "2223                               Jill Engle  \n",
       "2224                             xinamarieuhl  \n",
       "2256                          JenniferRutters  \n",
       "2270                               constatine  \n",
       "2279                             Maria Kasper  \n",
       "2282                             Robert Snoza  \n",
       "2289                        Jason Bolestridge  \n",
       "2290                                 Theodulf  \n",
       "2292                                   Arnold  \n",
       "2293                                drakaunus  \n",
       "2298                              Sheila Wood  \n",
       "2326                           Leanne Kinkopf  \n",
       "2328                               Adam Picot  \n",
       "2330                            David Jenkins  \n",
       "2331                             Madam Fickle  \n",
       "2348                           David Mecionis  \n",
       "2352                               Jaimie Noy  \n",
       "2356                             Kathy Wright  \n",
       "2377                              Jon Kissack  \n",
       "2381                             Larry Wilson  \n",
       "2389                                thecheops  \n",
       "2392                              Michele Fry  \n",
       "2394                              TinaNygard2  \n",
       "2396                            Jennifer Dorr  \n",
       "2414                           Michelle Goode  \n",
       "2419                                Gary Dana  \n",
       "2425                              noblesavage  \n",
       "2428                            Shivansh Dhar  \n",
       "2429                                  Eduardo  \n",
       "2451                            DeanOBuchanan  \n",
       "2455                             Paul Simonin  \n",
       "2462                            Sean Grabosky  \n",
       "2469                              Kevin Owens  \n",
       "2470                                   Mary J  \n",
       "2479                              Daisy Flaim  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers_file = \"LibriSpeech/SPEAKERS.TXT\"\n",
    "speakers = pd.read_table(\n",
    "    speakers_file,\n",
    "    engine='python',\n",
    "    sep='\\s+\\|\\s+',\n",
    "    names=['id','gender','subset','duration','name'],\n",
    "    dtype={'id': 'i','gender': 'U1', 'subset': 'U', 'duration': 'f', 'name': 'U'}, comment=';')\n",
    "speakers = speakers.drop(speakers[\n",
    "    (speakers['subset']=='train-clean-360')\n",
    "    |(speakers['subset']=='train-other-500')\n",
    "    |(speakers['subset']=='dev-other')\n",
    "    |(speakers['subset']=='test-other')\n",
    "].index)\n",
    "speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be5532f-c925-4e54-a998-40f9b7d6e609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subset\n",
       "train-clean-100    251\n",
       "test-clean          40\n",
       "dev-clean           40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers['subset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4f897a-3dd7-45b6-b92e-3261ff731837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0016.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0019.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0059.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0011.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19218-0005.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33857</th>\n",
       "      <td>2428</td>\n",
       "      <td>LibriSpeech/dev-clean/2428/83705/2428-83705-0019.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33858</th>\n",
       "      <td>2428</td>\n",
       "      <td>LibriSpeech/dev-clean/2428/83705/2428-83705-0028.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33859</th>\n",
       "      <td>2428</td>\n",
       "      <td>LibriSpeech/dev-clean/2428/83705/2428-83705-0025.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33860</th>\n",
       "      <td>2428</td>\n",
       "      <td>LibriSpeech/dev-clean/2428/83705/2428-83705-0021.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33861</th>\n",
       "      <td>2428</td>\n",
       "      <td>LibriSpeech/dev-clean/2428/83705/2428-83705-0036.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33862 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       speaker_id                                                         file\n",
       "0            5393  LibriSpeech/train-clean-100/5393/19218/5393-19218-0016.flac\n",
       "1            5393  LibriSpeech/train-clean-100/5393/19218/5393-19218-0019.flac\n",
       "2            5393  LibriSpeech/train-clean-100/5393/19218/5393-19218-0059.flac\n",
       "3            5393  LibriSpeech/train-clean-100/5393/19218/5393-19218-0011.flac\n",
       "4            5393  LibriSpeech/train-clean-100/5393/19218/5393-19218-0005.flac\n",
       "...           ...                                                          ...\n",
       "33857        2428        LibriSpeech/dev-clean/2428/83705/2428-83705-0019.flac\n",
       "33858        2428        LibriSpeech/dev-clean/2428/83705/2428-83705-0028.flac\n",
       "33859        2428        LibriSpeech/dev-clean/2428/83705/2428-83705-0025.flac\n",
       "33860        2428        LibriSpeech/dev-clean/2428/83705/2428-83705-0021.flac\n",
       "33861        2428        LibriSpeech/dev-clean/2428/83705/2428-83705-0036.flac\n",
       "\n",
       "[33862 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "filelist = Path('LibriSpeech').rglob('*.flac')\n",
    "files = [{'speaker_id': int(str(file).split('/')[2]), 'file': file} for file in filelist]\n",
    "df_files=pd.DataFrame(files)\n",
    "df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41d3a12-d7b2-4a69-b8dc-9354adc867ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_ml.model_development import train_valid_test_split\n",
    "\n",
    "df_files[\"Sets\"] = \"Training\"\n",
    "for lbl in df_files['speaker_id'].unique():\n",
    "    temp_data = df_files[df_files['speaker_id'] == lbl]\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_valid_test_split(\n",
    "        temp_data,\n",
    "        target=\"speaker_id\",\n",
    "        train_size=0.6,\n",
    "        valid_size=0.1,\n",
    "        test_size=0.3\n",
    "    )\n",
    "    df_files.Sets.iloc[X_test.index] = \"Testing\"\n",
    "    df_files.Sets.iloc[X_val.index] = \"Validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f25e7e-2802-4cb8-abdc-eb9f3915769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although this function was modified and many parameteres were explored with, most of it\n",
    "# came from Source 8 (sources in the READ.ME)\n",
    "\n",
    "def extract_features(files):\n",
    "    \n",
    "    # Sets the name to be the path to where the file is in my computer\n",
    "    file_name = str(files.file)\n",
    "\n",
    "    # Loads the audio file as a floating point time series and assigns the default sample rate\n",
    "    # Sample rate is set to 22050 by default\n",
    "    X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "\n",
    "    # Generate Mel-frequency cepstral coefficients (MFCCs) from a time series \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "\n",
    "    # Generates a Short-time Fourier transform (STFT) to use in the chroma_stft\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "\n",
    "    # Computes a chromagram from a waveform or power spectrogram.\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    # Computes a mel-scaled spectrogram.\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    # Computes spectral contrast\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "\n",
    "    # Computes the tonal centroid features (tonnetz)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),\n",
    "    sr=sample_rate).T,axis=0)\n",
    "        \n",
    "    \n",
    "    # We add also the classes of each file as a label at the end\n",
    "    label = files.speaker_id\n",
    "\n",
    "    return mfccs, chroma, mel, contrast, tonnetz, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee543457-1d46-403c-8637-9cb3b1561ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files.to_pickle('Librispeech_PyTorch_df_files.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02e8a07f-72d4-4ad1-a8dc-c4f652cb57d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = pd.read_pickle('Librispeech_PyTorch_df_files.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72b3d3e-9b8e-4577-8190-ce9e2e5e01da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading features from Librispeach_PyTorch_features_label.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "picklefile = 'Librispeach_PyTorch_features_label.pkl'\n",
    "if os.path.isfile(picklefile):\n",
    "    print(f\"Reading features from {picklefile}\")\n",
    "    features_label = pd.read_pickle(picklefile)\n",
    "else:\n",
    "    startTime = datetime.now()\n",
    "    features_label = df_files.apply(extract_features, axis=1)\n",
    "    print(datetime.now() - startTime)\n",
    "    features_label.to_pickle(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab5334d-a2c4-4ba3-89c0-d438f9e65b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ([-355.78824, 115.00196, -33.891296, 38.743874, -9.843966, 1.6396469, -3.4062624, -5.2952833, 9.099011, -2.4850764, 0.7190697, -3.04304, 5.025559, 3.5158784, -1.4026278, -0.12272247, 2.450649, -0.13805504, -6.4034905, 0.22591878, -1.1307915, -4.3972487, -1.0534327, 0.05638659, -2.9429982, -1.1572398, -0.08161958, -1.9492955, 0.79942846, -0.13769592, 1.6648215, 3.1556494, 3.4880686, 4.2836747, 2.9823754, 3.6624057, 2.6771302, 2.9374325, 2.6298814, 1.1149267], [0.6906988, 0.6511583, 0.58476067, 0.5434637, 0.555994, 0.57990086, 0.61002755, 0.61547655, 0.62042934, 0.6521874, 0.669347, 0.6868791], [0.041331366, 0.19435953, 0.088610314, 0.010506179, 0.031644356, 0.47447613, 2.575366, 5.955543, 3.446617, 0.3173315, 0.06681397, 0.08960947, 0.48753977, 0.86967057, 1.6997426, 2.862485, 7.4286966, 7.401466, 0.4023322, 1.0836115, 0.9524352, 1.0556357, 1.2197704, 1.6775879, 1.8304838, 0.66789055, 0.38433993, 0.69279563, 0.95153636, 3.043987, 1.3170439, 1.9210573, 3.3581932, 1.4897953, 0.76316136, 0.97512656, 1.7829665, 1.6641381, 0.9348044, 1.8074384, 1.756828, 0.5011097, 0.25429305, 0.30603325, 0.24565358, 0.29467925, 0.964659, 0.790962, 0.31089506, 0.32311475, 0.2772574, 0.17674926, 0.16505414, 0.09894319, 0.07692517, 0.036325518, 0.024993554, 0.04703603, 0.08582365, 0.104880065, 0.05337718, 0.071079016, 0.19209135, 0.17987211, 0.043461863, 0.071494535, 0.122341454, 0.04417352, 0.017518021, 0.017243974, 0.014966248, 0.008801299, 0.012679274, 0.014887435, 0.023177173, 0.019582491, 0.022329314, 0.020180807, 0.036628738, 0.04189545, 0.031362507, 0.02739021, 0.014030596, 0.00930112, 0.01159567, 0.020765767, 0.025088156, 0.021724567, 0.021170055, 0.019242248, 0.017241433, 0.014011418, 0.011443389, 0.00839163, 0.005569583, 0.0046077752, 0.002988828, 0.003394586, 0.0036774988, 0.002011965, ...], [18.103818359989667, 13.739479281938337, 16.233402744596603, 15.768192202832422, 16.936454507405706, 19.095880927077427, 52.021207809531795], [-0.05637205903946352, 0.0031072485504250627, 0.04249068415197286, -0.0949989153153227, 0.03170013370623189, -0.026193645854642372], 5393)\n",
       "1    ([-339.40924, 126.59402, -35.345768, 42.477505, -8.09071, 5.9230247, -4.6247625, -11.421107, 4.111699, -3.8405964, 2.0892882, -3.5739326, 4.8683395, 5.9477086, -1.8131262, -1.7914606, 3.6542513, -0.64018846, -5.111973, 0.51264656, -3.4217613, -2.158121, -0.15317032, -1.4979421, -2.1373162, -0.7568513, 0.72938234, -0.036075577, 0.692292, -0.7059956, -0.2618313, 0.46078762, 0.55698186, 1.2042918, 2.7445998, 2.3302321, 1.6183771, 2.618989, 0.7988489, 1.6607175], [0.6196765, 0.5857673, 0.55671835, 0.5564703, 0.5939191, 0.59412456, 0.6016075, 0.61330026, 0.6191192, 0.59130245, 0.60358644, 0.6463599], [0.07689863, 0.30681008, 0.16080259, 0.026984813, 0.09331438, 1.1005687, 2.6777172, 3.8408682, 2.5828483, 1.8424523, 0.83260244, 0.82894725, 1.7404809, 1.1793017, 1.7826971, 2.0626936, 2.5399325, 4.20851, 3.2561753, 1.6838726, 1.2287047, 1.0106905, 1.1329355, 1.4203564, 1.1543841, 0.65893924, 0.95222723, 1.1715463, 0.83532476, 0.81106246, 0.50551176, 1.1761894, 0.5617408, 0.19628094, 0.23470072, 0.4636167, 0.93869436, 1.1186743, 0.33797383, 0.45299482, 0.19184202, 0.08266714, 0.095605224, 0.3422577, 0.32142097, 0.1818646, 0.29343235, 0.1697926, 0.15697405, 0.20123705, 0.14773367, 0.1538313, 0.23511192, 0.12711236, 0.12856224, 0.12265602, 0.14758079, 0.2010161, 0.19372948, 0.1951936, 0.15424632, 0.21689793, 0.13766904, 0.12474099, 0.08926489, 0.063202, 0.052533925, 0.030250255, 0.021927528, 0.025959525, 0.019414958, 0.016414717, 0.017856678, 0.01941584, 0.021873182, 0.027062528, 0.021343088, 0.022161268, 0.032791995, 0.036091153, 0.038484566, 0.037189346, 0.030310815, 0.027094528, 0.018318314, 0.017860081, 0.019590557, 0.01715106, 0.01835027, 0.016982252, 0.016466439, 0.011808807, 0.0113426065, 0.01215744, 0.012495731, 0.008861239, 0.007195422, 0.005013637, 0.0036859133, 0.0028408254, ...], [20.359853039471453, 15.977658311063083, 17.84901944532795, 15.932706760711582, 17.870117687859977, 19.732840382184452, 52.56849435895738], [-0.07494528883225615, -0.010583872618476086, 0.10347697377459676, -0.06207722487051121, 0.029365634281267466, -0.03780028639605072], 5393)\n",
       "2                                       ([-353.02054, 126.88507, -33.101448, 31.081549, 1.248003, -0.8467134, -4.9822803, -8.124212, 6.4226966, -3.1726806, -0.26687703, 2.5708115, 2.7887962, 2.8970487, 0.16667502, -2.2747548, 1.3617232, 1.2741829, -3.8738756, -3.0060735, -0.86629385, -0.6689545, -0.52347875, 1.7157615, -0.21120831, 0.3583978, 1.7900268, 0.7974127, 1.1512038, 0.39606738, 1.7015365, 1.4480557, 1.5063242, 0.91252476, -0.53619194, -0.12851945, 0.31157133, 1.9322026, 1.7099967, 1.3882828], [0.7189085, 0.6698097, 0.62975794, 0.62287587, 0.6194952, 0.60062706, 0.58804667, 0.57452404, 0.61365074, 0.6613425, 0.69924766, 0.71696967], [0.06283332, 0.22804034, 0.09545967, 0.016912386, 0.104812816, 0.941729, 2.3540401, 4.2946467, 4.638273, 4.9607353, 3.7880313, 4.299104, 3.0663798, 2.0638063, 1.860784, 3.6255958, 4.6155562, 3.647604, 4.806401, 9.001531, 7.085616, 4.823719, 4.978298, 4.780242, 4.5283403, 5.051439, 2.9710288, 1.7398711, 1.2113916, 1.480172, 1.168451, 1.0854474, 1.7824314, 1.7342124, 1.6531272, 0.68872195, 0.4935337, 0.45536855, 0.4720197, 0.6886391, 0.7251399, 0.7975678, 0.6285441, 0.43903542, 0.36274138, 0.312696, 0.3527581, 0.40182304, 0.53526014, 0.51858985, 0.36217383, 0.3426197, 0.2666379, 0.33242205, 0.4165807, 0.5456534, 0.56489134, 0.50392455, 0.5395852, 0.7090634, 1.1393121, 0.58888596, 0.48423687, 0.40131655, 0.37609267, 0.2988943, 0.26482224, 0.2533392, 0.17478542, 0.1531301, 0.14026861, 0.114045896, 0.07511252, 0.06840159, 0.07813431, 0.073544204, 0.07297666, 0.08888897, 0.08114717, 0.054569025, 0.04817891, 0.032912876, 0.02569516, 0.027167361, 0.014502966, 0.0140607245, 0.016849853, 0.015733277, 0.017648334, 0.019348076, 0.020375455, 0.016586576, 0.0137354545, 0.013012001, 0.009065444, 0.007082726, 0.0044586845, 0.0027871435, 0.0023098474, 0.0018295717, ...], [19.650946696118947, 16.254519966552905, 17.10735876882015, 15.54553353905934, 17.52552118179907, 19.959115300074206, 50.58196811116587], [-0.09487818047369138, 0.04323787709581943, 0.04081876346294587, -0.13303730762134755, 0.03095253942273878, -0.04912160353275169], 5393)\n",
       "3                                                   ([-299.06805, 119.90098, -51.21718, 53.760983, -13.989659, 4.513041, -9.086183, -6.220014, 6.069344, -7.892628, 1.4903105, -5.0083594, 5.181691, 2.5311916, -0.40027535, -2.8805425, 2.2314205, -0.6110487, -6.4190216, 0.6564069, -3.5355296, 0.10292023, 1.341769, 2.4365902, 1.0023263, 0.5901412, 0.9686754, 0.025761219, 2.714789, 0.69423914, 3.7991157, 1.5028179, 0.027816556, 0.23328668, -1.8180349, -0.40234476, -0.25462097, 1.3110031, 0.85441244, 1.0659148], [0.61948735, 0.5762995, 0.5667317, 0.5641471, 0.5768054, 0.5680462, 0.535634, 0.5471955, 0.5822229, 0.591095, 0.5908144, 0.61386263], [0.049457908, 0.24891534, 0.12337868, 0.020403367, 0.099318795, 1.5428457, 2.980053, 5.421191, 9.790094, 6.5797625, 6.1741505, 9.377708, 3.4051216, 1.4443649, 1.5068882, 2.172291, 5.835434, 5.4585648, 5.0770106, 5.161765, 4.0953016, 2.1591659, 2.775545, 4.8651423, 3.1688056, 1.7817917, 1.8335224, 2.6243937, 1.6975621, 1.8813612, 1.4662938, 1.1215028, 0.9807772, 2.6927207, 2.025438, 1.6603895, 1.3734787, 1.3660724, 1.4601128, 1.4467231, 1.2642082, 1.7396338, 1.9241322, 1.2713795, 1.1612256, 0.85070366, 0.44029352, 0.3803607, 0.47848117, 0.3399562, 0.232072, 0.3465071, 0.3418815, 0.33590716, 0.41125706, 0.30511522, 0.5438386, 0.5810559, 0.5370785, 0.8677315, 0.7269609, 1.0575787, 0.86460316, 0.6051784, 0.5923161, 0.50344247, 0.6049989, 0.42901617, 0.085777365, 0.07719278, 0.12972465, 0.14365703, 0.08074784, 0.056803446, 0.075492084, 0.08013532, 0.091001414, 0.09097808, 0.10689492, 0.08812778, 0.08537623, 0.06348197, 0.047418725, 0.055072192, 0.030862944, 0.024128659, 0.024334842, 0.024374515, 0.022419684, 0.027693722, 0.033822324, 0.035366528, 0.037889052, 0.033278827, 0.027817762, 0.022842102, 0.015774496, 0.009471169, 0.0090397075, 0.0096254395, ...], [19.542196185707784, 17.052741245788315, 18.53081908351417, 16.024487658493392, 18.09657444017647, 20.156469942641245, 53.36904076724722], [-0.049093563513895884, 0.026678667681878664, 0.009638179579956264, -0.09654374714526907, 0.021372461258758805, -0.025102454911296448], 5393)\n",
       "4                      ([-336.197, 125.143036, -29.488451, 30.32312, -0.94586504, 5.353153, -7.0330772, -11.134182, 5.3797426, -2.6209729, 0.31341526, -0.8746875, 4.992316, 5.22345, -2.132455, -1.937337, 3.4545453, -1.4140024, -5.2569966, -1.440852, -2.9579048, -2.4127038, -1.7587613, -0.22046067, -2.5331764, -1.3470275, 0.30108374, -1.0937754, 1.3872992, 0.43354487, 2.1473844, 1.4508853, 0.7769986, 2.906713, 3.3096168, 5.168283, 5.339414, 4.8413067, 2.4629943, 0.6396179], [0.59551036, 0.60055864, 0.53741235, 0.5125383, 0.53680414, 0.5978921, 0.6474819, 0.6218445, 0.5987949, 0.6044882, 0.6202751, 0.600321], [0.055100374, 0.33825466, 0.21384524, 0.052399613, 0.041223872, 0.5410578, 4.9715524, 7.016913, 4.828829, 1.775615, 0.30062315, 0.18317689, 0.62101775, 2.1967258, 4.795426, 3.7186806, 4.8531013, 6.3311524, 5.168964, 2.744516, 3.184556, 4.0871854, 2.4187171, 1.66923, 1.5642574, 1.5384328, 1.3109195, 3.3430293, 2.4153318, 0.7848423, 0.8021677, 0.78443193, 0.43250796, 0.56894547, 1.1730199, 1.1038156, 1.1240482, 1.0566156, 0.527064, 0.37298346, 0.40731177, 0.7355084, 0.88915896, 0.6776392, 0.4397021, 0.5208453, 0.63295394, 0.44301623, 0.40096435, 0.39425412, 0.23611985, 0.4581201, 0.39899528, 0.22265275, 0.14953053, 0.12762228, 0.1394563, 0.17723751, 0.1905953, 0.11531855, 0.13546443, 0.26568666, 0.22133349, 0.098850206, 0.1328014, 0.18380158, 0.15139647, 0.053398017, 0.036138866, 0.035882596, 0.043657668, 0.03660349, 0.016522173, 0.017956883, 0.019996343, 0.011542829, 0.01390054, 0.018561544, 0.016907018, 0.015923606, 0.012985859, 0.012774781, 0.009808206, 0.008422991, 0.0064091585, 0.0048045614, 0.006296964, 0.0065956158, 0.009091342, 0.00771468, 0.008879445, 0.008282698, 0.0077021252, 0.0058676135, 0.004280776, 0.004096645, 0.0031162943, 0.002886946, 0.0028396994, 0.0025574276, ...], [20.958073629073766, 15.880909260378502, 18.026739100556853, 16.054670001972188, 18.181791805634504, 19.359464162875383, 53.06566121993242], [0.0379831478370396, -0.09563648588977362, 0.14651702134765424, -0.10384795463694163, -0.018761609050515198, -0.03900062308289822], 5393)\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7e3629-756b-4c28-a732-228fb0929735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33862,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8317ebcd-463d-432c-922f-909b183070c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features = []\n",
    "for i in range(0, len(features_label)):\n",
    "    features.append(np.concatenate((features_label[i][0], features_label[i][1], \n",
    "                features_label[i][2], features_label[i][3],\n",
    "                features_label[i][4]), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adb55ff2-407f-45da-a127-65834fe922e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files['X'] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47e72217-4b08-48e2-bead-0185805d600f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33862])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "speakers_tensor = torch.tensor(df_files['speaker_id'])\n",
    "speakers_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94ac05c0-5d8a-4590-9b4f-87f03c471eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_onehot = torch.zeros(speakers_tensor.shape[0], 331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4808d286-8e5e-4cf0-a689-2fe76cb261ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2index = {label:idx for idx,label in enumerate(set(df_files['speaker_id']))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2498fa30-0cd0-417a-8612-29c10a55fa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "331"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49a951de-04c7-4d55-ac58-b85c58b1b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_labels = [label2index[label] for label in df_files['speaker_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "199a2969-d8b3-4681-9a12-79a104dfbf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 228,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 284,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 84,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 273,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 316,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 306,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " 299,\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f9f7da4-5156-43f8-ab8c-5c7ede206100",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(label2index)\n",
    "one_hot_encoded = torch.nn.functional.one_hot(torch.tensor(numeric_labels), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "360b63ee-bff7-47e5-9e29-93adccfaf6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33862, 331])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "91e52234-c31f-42a3-a1db-f77f7cb4d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files = df_files.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c96f15a-dd06-4eec-9192-85cbc14f1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files['y'] = one_hot_encoded.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a42065a-dd9d-4876-8244-07c85c1e2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_files['y'] = numeric_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c735ed5b-b4ae-45eb-b805-251fd32091d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>file</th>\n",
       "      <th>Sets</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[-355.7882385253906, 115.00196075439453, -33.8...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19...</td>\n",
       "      <td>Training</td>\n",
       "      <td>[-339.40924072265625, 126.5940170288086, -35.3...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19...</td>\n",
       "      <td>Validation</td>\n",
       "      <td>[-353.0205383300781, 126.88507080078125, -33.1...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19...</td>\n",
       "      <td>Validation</td>\n",
       "      <td>[-299.06805419921875, 119.9009780883789, -51.2...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5393</td>\n",
       "      <td>LibriSpeech/train-clean-100/5393/19218/5393-19...</td>\n",
       "      <td>Testing</td>\n",
       "      <td>[-336.1969909667969, 125.14303588867188, -29.4...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   speaker_id                                               file        Sets  \\\n",
       "0        5393  LibriSpeech/train-clean-100/5393/19218/5393-19...    Training   \n",
       "1        5393  LibriSpeech/train-clean-100/5393/19218/5393-19...    Training   \n",
       "2        5393  LibriSpeech/train-clean-100/5393/19218/5393-19...  Validation   \n",
       "3        5393  LibriSpeech/train-clean-100/5393/19218/5393-19...  Validation   \n",
       "4        5393  LibriSpeech/train-clean-100/5393/19218/5393-19...     Testing   \n",
       "\n",
       "                                                   X    y  \n",
       "0  [-355.7882385253906, 115.00196075439453, -33.8...  228  \n",
       "1  [-339.40924072265625, 126.5940170288086, -35.3...  228  \n",
       "2  [-353.0205383300781, 126.88507080078125, -33.1...  228  \n",
       "3  [-299.06805419921875, 119.9009780883789, -51.2...  228  \n",
       "4  [-336.1969909667969, 125.14303588867188, -29.4...  228  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ba6c7cc-7b93-4235-a0e3-cee1fa6a8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(np.array(df_files[df_files['Sets']=='Training']['X'].tolist()))\n",
    "y_train = np.array(df_files[df_files['Sets']=='Training']['y'].tolist())\n",
    "X_val = ss.fit_transform(np.array(df_files[df_files['Sets']=='Validation']['X'].tolist()))\n",
    "y_val = np.array(df_files[df_files['Sets']=='Validation']['y'].tolist())\n",
    "X_test = ss.fit_transform(np.array(df_files[df_files['Sets']=='Testing']['X'].tolist()))\n",
    "y_test = np.array(df_files[df_files['Sets']=='Testing']['y'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70454c96-3e8c-4cf7-9ccb-06e8b3f34aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20183, 193)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb19d73d-5e96-4c85-b87f-284203411166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20183,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe48aacd-828b-4cfe-b5b7-82405fabe125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19be8bd1-92f8-43a1-9928-857e5fa33da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, parameters, labels, device):\n",
    "        self.parameters = parameters\n",
    "        self.labels = labels\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.parameters)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        param = torch.tensor(self.parameters[idx], dtype=torch.float32).to(device)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long).to(device)\n",
    "        return param, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92031014-3833-4cf9-bbe6-dfbbb6a9a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Data: tensor([[-0.3111,  0.0318, -0.4333,  ..., -1.6627,  1.8160, -0.9501],\n",
      "        [ 0.1062,  0.6596, -0.5080,  ..., -0.9844,  1.6718, -1.5117],\n",
      "        [ 0.2290,  0.3779, -0.5216,  ..., -1.5478,  0.1473, -1.4794],\n",
      "        ...,\n",
      "        [ 0.3495,  0.3239, -0.6233,  ...,  1.0778, -1.9075, -1.1685],\n",
      "        [-0.0137,  0.4033, -0.6486,  ..., -1.5458,  0.1171, -1.7513],\n",
      "        [ 0.5422, -0.1370, -0.1985,  ..., -0.2513,  0.2013, -1.8000]])\n",
      "Targets: tensor([228, 228, 228, 228, 228, 228, 228, 228, 228, 228])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device {device}\")\n",
    "train_loader = torch.utils.data.DataLoader(CustomDataset(X_train, y_train, device), batch_size=10)\n",
    "for data, targets in train_loader:\n",
    "    print(f\"Data: {data}\")\n",
    "    print(f\"Targets: {targets}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "111636df-306e-4c15-b959-6b4970b89219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(193, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 331)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.softmax(x)\n",
    "        #print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ad1c9fe-b80e-42f4-a816-c6e74d04491d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92040b3e-b83b-4073-af98-1478fd94a500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=193, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=331, bias=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.25, inplace=False)\n",
       "  (dropout3): Dropout(p=0.5, inplace=False)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cf8c042-14fd-466b-b10d-00f1ee3b3ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Epoch: 1/200\n",
      "Train Loss: 5.3746, Train Accuracy: 0.4324\n",
      "Validation Loss: 4.9407, Validation Accuracy: 0.8679\n",
      "0.001\n",
      "Epoch: 2/200\n",
      "Train Loss: 5.3703, Train Accuracy: 0.4369\n",
      "Validation Loss: 4.9399, Validation Accuracy: 0.8694\n",
      "0.001\n",
      "Epoch: 3/200\n",
      "Train Loss: 5.3726, Train Accuracy: 0.4340\n",
      "Validation Loss: 4.9380, Validation Accuracy: 0.8709\n",
      "0.001\n",
      "Epoch: 4/200\n",
      "Train Loss: 5.3694, Train Accuracy: 0.4374\n",
      "Validation Loss: 4.9377, Validation Accuracy: 0.8724\n",
      "0.001\n",
      "Epoch: 5/200\n",
      "Train Loss: 5.3601, Train Accuracy: 0.4471\n",
      "Validation Loss: 4.9325, Validation Accuracy: 0.8761\n",
      "0.001\n",
      "Epoch: 6/200\n",
      "Train Loss: 5.3700, Train Accuracy: 0.4367\n",
      "Validation Loss: 4.9318, Validation Accuracy: 0.8776\n",
      "0.001\n",
      "Epoch: 7/200\n",
      "Train Loss: 5.3677, Train Accuracy: 0.4400\n",
      "Validation Loss: 4.9280, Validation Accuracy: 0.8821\n",
      "0.001\n",
      "Epoch: 8/200\n",
      "Train Loss: 5.3624, Train Accuracy: 0.4446\n",
      "Validation Loss: 4.9272, Validation Accuracy: 0.8818\n",
      "0.001\n",
      "Epoch: 9/200\n",
      "Train Loss: 5.3585, Train Accuracy: 0.4482\n",
      "Validation Loss: 4.9267, Validation Accuracy: 0.8827\n",
      "0.001\n",
      "Epoch: 10/200\n",
      "Train Loss: 5.3638, Train Accuracy: 0.4430\n",
      "Validation Loss: 4.9267, Validation Accuracy: 0.8827\n",
      "0.001\n",
      "Epoch: 11/200\n",
      "Train Loss: 5.3669, Train Accuracy: 0.4399\n",
      "Validation Loss: 4.9241, Validation Accuracy: 0.8845\n",
      "0.001\n",
      "Epoch: 12/200\n",
      "Train Loss: 5.3572, Train Accuracy: 0.4496\n",
      "Validation Loss: 4.9226, Validation Accuracy: 0.8861\n",
      "0.001\n",
      "Epoch: 13/200\n",
      "Train Loss: 5.3539, Train Accuracy: 0.4534\n",
      "Validation Loss: 4.9203, Validation Accuracy: 0.8891\n",
      "0.001\n",
      "Epoch: 14/200\n",
      "Train Loss: 5.3579, Train Accuracy: 0.4490\n",
      "Validation Loss: 4.9171, Validation Accuracy: 0.8921\n",
      "0.001\n",
      "Epoch: 15/200\n",
      "Train Loss: 5.3541, Train Accuracy: 0.4523\n",
      "Validation Loss: 4.9177, Validation Accuracy: 0.8912\n",
      "0.001\n",
      "Epoch: 16/200\n",
      "Train Loss: 5.3632, Train Accuracy: 0.4437\n",
      "Validation Loss: 4.9136, Validation Accuracy: 0.8961\n",
      "0.001\n",
      "Epoch: 17/200\n",
      "Train Loss: 5.3546, Train Accuracy: 0.4518\n",
      "Validation Loss: 4.9144, Validation Accuracy: 0.8945\n",
      "0.001\n",
      "Epoch: 18/200\n",
      "Train Loss: 5.3546, Train Accuracy: 0.4522\n",
      "Validation Loss: 4.9148, Validation Accuracy: 0.8939\n",
      "0.001\n",
      "Epoch: 19/200\n",
      "Train Loss: 5.3596, Train Accuracy: 0.4478\n",
      "Validation Loss: 4.9138, Validation Accuracy: 0.8945\n",
      "0.001\n",
      "Epoch: 20/200\n",
      "Train Loss: 5.3530, Train Accuracy: 0.4535\n",
      "Validation Loss: 4.9134, Validation Accuracy: 0.8952\n",
      "0.001\n",
      "Epoch: 21/200\n",
      "Train Loss: 5.3523, Train Accuracy: 0.4543\n",
      "Validation Loss: 4.9116, Validation Accuracy: 0.8961\n",
      "0.001\n",
      "Epoch: 22/200\n",
      "Train Loss: 5.3540, Train Accuracy: 0.4529\n",
      "Validation Loss: 4.9129, Validation Accuracy: 0.8955\n",
      "0.001\n",
      "Epoch: 23/200\n",
      "Train Loss: 5.3603, Train Accuracy: 0.4463\n",
      "Validation Loss: 4.9136, Validation Accuracy: 0.8955\n",
      "0.001\n",
      "Epoch: 24/200\n",
      "Train Loss: 5.3556, Train Accuracy: 0.4506\n",
      "Validation Loss: 4.9119, Validation Accuracy: 0.8970\n",
      "0.001\n",
      "Epoch: 25/200\n",
      "Train Loss: 5.3561, Train Accuracy: 0.4506\n",
      "Validation Loss: 4.9123, Validation Accuracy: 0.8955\n",
      "0.001\n",
      "Epoch: 26/200\n",
      "Train Loss: 5.3518, Train Accuracy: 0.4547\n",
      "Validation Loss: 4.9131, Validation Accuracy: 0.8958\n",
      "0.001\n",
      "Epoch: 27/200\n",
      "Train Loss: 5.3611, Train Accuracy: 0.4449\n",
      "Validation Loss: 4.9117, Validation Accuracy: 0.8973\n",
      "0.001\n",
      "Epoch: 28/200\n",
      "Train Loss: 5.3561, Train Accuracy: 0.4499\n",
      "Validation Loss: 4.9111, Validation Accuracy: 0.8976\n",
      "0.001\n",
      "Epoch: 29/200\n",
      "Train Loss: 5.3594, Train Accuracy: 0.4466\n",
      "Validation Loss: 4.9122, Validation Accuracy: 0.8970\n",
      "0.001\n",
      "Epoch: 30/200\n",
      "Train Loss: 5.3574, Train Accuracy: 0.4494\n",
      "Validation Loss: 4.9112, Validation Accuracy: 0.8973\n",
      "0.001\n",
      "Epoch: 31/200\n",
      "Train Loss: 5.3570, Train Accuracy: 0.4491\n",
      "Validation Loss: 4.9098, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 32/200\n",
      "Train Loss: 5.3586, Train Accuracy: 0.4475\n",
      "Validation Loss: 4.9091, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 33/200\n",
      "Train Loss: 5.3535, Train Accuracy: 0.4526\n",
      "Validation Loss: 4.9087, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 34/200\n",
      "Train Loss: 5.3548, Train Accuracy: 0.4515\n",
      "Validation Loss: 4.9090, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 35/200\n",
      "Train Loss: 5.3529, Train Accuracy: 0.4538\n",
      "Validation Loss: 4.9091, Validation Accuracy: 0.8991\n",
      "0.001\n",
      "Epoch: 36/200\n",
      "Train Loss: 5.3552, Train Accuracy: 0.4513\n",
      "Validation Loss: 4.9084, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 37/200\n",
      "Train Loss: 5.3592, Train Accuracy: 0.4471\n",
      "Validation Loss: 4.9078, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 38/200\n",
      "Train Loss: 5.3516, Train Accuracy: 0.4548\n",
      "Validation Loss: 4.9084, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 39/200\n",
      "Train Loss: 5.3544, Train Accuracy: 0.4523\n",
      "Validation Loss: 4.9090, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 40/200\n",
      "Train Loss: 5.3575, Train Accuracy: 0.4487\n",
      "Validation Loss: 4.9106, Validation Accuracy: 0.8973\n",
      "0.001\n",
      "Epoch: 41/200\n",
      "Train Loss: 5.3542, Train Accuracy: 0.4520\n",
      "Validation Loss: 4.9099, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 42/200\n",
      "Train Loss: 5.3596, Train Accuracy: 0.4472\n",
      "Validation Loss: 4.9087, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 43/200\n",
      "Train Loss: 5.3509, Train Accuracy: 0.4553\n",
      "Validation Loss: 4.9099, Validation Accuracy: 0.8982\n",
      "0.001\n",
      "Epoch: 44/200\n",
      "Train Loss: 5.3557, Train Accuracy: 0.4507\n",
      "Validation Loss: 4.9107, Validation Accuracy: 0.8979\n",
      "0.001\n",
      "Epoch: 45/200\n",
      "Train Loss: 5.3519, Train Accuracy: 0.4536\n",
      "Validation Loss: 4.9099, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 46/200\n",
      "Train Loss: 5.3514, Train Accuracy: 0.4553\n",
      "Validation Loss: 4.9085, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 47/200\n",
      "Train Loss: 5.3495, Train Accuracy: 0.4568\n",
      "Validation Loss: 4.9085, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 48/200\n",
      "Train Loss: 5.3522, Train Accuracy: 0.4542\n",
      "Validation Loss: 4.9073, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 49/200\n",
      "Train Loss: 5.3523, Train Accuracy: 0.4540\n",
      "Validation Loss: 4.9086, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 50/200\n",
      "Train Loss: 5.3484, Train Accuracy: 0.4584\n",
      "Validation Loss: 4.9068, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 51/200\n",
      "Train Loss: 5.3484, Train Accuracy: 0.4579\n",
      "Validation Loss: 4.9072, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 52/200\n",
      "Train Loss: 5.3610, Train Accuracy: 0.4451\n",
      "Validation Loss: 4.9081, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 53/200\n",
      "Train Loss: 5.3542, Train Accuracy: 0.4523\n",
      "Validation Loss: 4.9082, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 54/200\n",
      "Train Loss: 5.3586, Train Accuracy: 0.4481\n",
      "Validation Loss: 4.9079, Validation Accuracy: 0.9006\n",
      "0.001\n",
      "Epoch: 55/200\n",
      "Train Loss: 5.3485, Train Accuracy: 0.4584\n",
      "Validation Loss: 4.9099, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 56/200\n",
      "Train Loss: 5.3545, Train Accuracy: 0.4520\n",
      "Validation Loss: 4.9107, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 57/200\n",
      "Train Loss: 5.3580, Train Accuracy: 0.4484\n",
      "Validation Loss: 4.9097, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 58/200\n",
      "Train Loss: 5.3499, Train Accuracy: 0.4569\n",
      "Validation Loss: 4.9102, Validation Accuracy: 0.8970\n",
      "0.001\n",
      "Epoch: 59/200\n",
      "Train Loss: 5.3543, Train Accuracy: 0.4517\n",
      "Validation Loss: 4.9085, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 60/200\n",
      "Train Loss: 5.3524, Train Accuracy: 0.4536\n",
      "Validation Loss: 4.9117, Validation Accuracy: 0.8964\n",
      "0.001\n",
      "Epoch: 61/200\n",
      "Train Loss: 5.3553, Train Accuracy: 0.4509\n",
      "Validation Loss: 4.9076, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 62/200\n",
      "Train Loss: 5.3508, Train Accuracy: 0.4560\n",
      "Validation Loss: 4.9091, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 63/200\n",
      "Train Loss: 5.3553, Train Accuracy: 0.4508\n",
      "Validation Loss: 4.9099, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 64/200\n",
      "Train Loss: 5.3544, Train Accuracy: 0.4518\n",
      "Validation Loss: 4.9080, Validation Accuracy: 0.9006\n",
      "0.001\n",
      "Epoch: 65/200\n",
      "Train Loss: 5.3524, Train Accuracy: 0.4539\n",
      "Validation Loss: 4.9090, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 66/200\n",
      "Train Loss: 5.3508, Train Accuracy: 0.4556\n",
      "Validation Loss: 4.9106, Validation Accuracy: 0.8979\n",
      "0.001\n",
      "Epoch: 67/200\n",
      "Train Loss: 5.3512, Train Accuracy: 0.4552\n",
      "Validation Loss: 4.9081, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 68/200\n",
      "Train Loss: 5.3525, Train Accuracy: 0.4533\n",
      "Validation Loss: 4.9086, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 69/200\n",
      "Train Loss: 5.3589, Train Accuracy: 0.4476\n",
      "Validation Loss: 4.9079, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 70/200\n",
      "Train Loss: 5.3515, Train Accuracy: 0.4545\n",
      "Validation Loss: 4.9072, Validation Accuracy: 0.9018\n",
      "0.001\n",
      "Epoch: 71/200\n",
      "Train Loss: 5.3545, Train Accuracy: 0.4517\n",
      "Validation Loss: 4.9077, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 72/200\n",
      "Train Loss: 5.3608, Train Accuracy: 0.4451\n",
      "Validation Loss: 4.9081, Validation Accuracy: 0.9006\n",
      "0.001\n",
      "Epoch: 73/200\n",
      "Train Loss: 5.3509, Train Accuracy: 0.4551\n",
      "Validation Loss: 4.9075, Validation Accuracy: 0.9006\n",
      "0.001\n",
      "Epoch: 74/200\n",
      "Train Loss: 5.3532, Train Accuracy: 0.4535\n",
      "Validation Loss: 4.9096, Validation Accuracy: 0.8982\n",
      "0.001\n",
      "Epoch: 75/200\n",
      "Train Loss: 5.3473, Train Accuracy: 0.4590\n",
      "Validation Loss: 4.9075, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 76/200\n",
      "Train Loss: 5.3565, Train Accuracy: 0.4507\n",
      "Validation Loss: 4.9091, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 77/200\n",
      "Train Loss: 5.3519, Train Accuracy: 0.4541\n",
      "Validation Loss: 4.9094, Validation Accuracy: 0.8979\n",
      "0.001\n",
      "Epoch: 78/200\n",
      "Train Loss: 5.3594, Train Accuracy: 0.4470\n",
      "Validation Loss: 4.9096, Validation Accuracy: 0.8982\n",
      "0.001\n",
      "Epoch: 79/200\n",
      "Train Loss: 5.3524, Train Accuracy: 0.4538\n",
      "Validation Loss: 4.9080, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 80/200\n",
      "Train Loss: 5.3532, Train Accuracy: 0.4537\n",
      "Validation Loss: 4.9077, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 81/200\n",
      "Train Loss: 5.3564, Train Accuracy: 0.4506\n",
      "Validation Loss: 4.9089, Validation Accuracy: 0.8991\n",
      "0.001\n",
      "Epoch: 82/200\n",
      "Train Loss: 5.3577, Train Accuracy: 0.4488\n",
      "Validation Loss: 4.9086, Validation Accuracy: 0.8994\n",
      "0.001\n",
      "Epoch: 83/200\n",
      "Train Loss: 5.3545, Train Accuracy: 0.4525\n",
      "Validation Loss: 4.9080, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 84/200\n",
      "Train Loss: 5.3493, Train Accuracy: 0.4572\n",
      "Validation Loss: 4.9084, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 85/200\n",
      "Train Loss: 5.3531, Train Accuracy: 0.4528\n",
      "Validation Loss: 4.9091, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 86/200\n",
      "Train Loss: 5.3553, Train Accuracy: 0.4510\n",
      "Validation Loss: 4.9078, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 87/200\n",
      "Train Loss: 5.3472, Train Accuracy: 0.4593\n",
      "Validation Loss: 4.9068, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 88/200\n",
      "Train Loss: 5.3492, Train Accuracy: 0.4570\n",
      "Validation Loss: 4.9080, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 89/200\n",
      "Train Loss: 5.3564, Train Accuracy: 0.4496\n",
      "Validation Loss: 4.9068, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 90/200\n",
      "Train Loss: 5.3545, Train Accuracy: 0.4520\n",
      "Validation Loss: 4.9060, Validation Accuracy: 0.9024\n",
      "0.001\n",
      "Epoch: 91/200\n",
      "Train Loss: 5.3526, Train Accuracy: 0.4534\n",
      "Validation Loss: 4.9064, Validation Accuracy: 0.9018\n",
      "0.001\n",
      "Epoch: 92/200\n",
      "Train Loss: 5.3531, Train Accuracy: 0.4529\n",
      "Validation Loss: 4.9062, Validation Accuracy: 0.9024\n",
      "0.001\n",
      "Epoch: 93/200\n",
      "Train Loss: 5.3536, Train Accuracy: 0.4523\n",
      "Validation Loss: 4.9067, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 94/200\n",
      "Train Loss: 5.3552, Train Accuracy: 0.4508\n",
      "Validation Loss: 4.9092, Validation Accuracy: 0.8979\n",
      "0.001\n",
      "Epoch: 95/200\n",
      "Train Loss: 5.3530, Train Accuracy: 0.4535\n",
      "Validation Loss: 4.9110, Validation Accuracy: 0.8973\n",
      "0.001\n",
      "Epoch: 96/200\n",
      "Train Loss: 5.3550, Train Accuracy: 0.4512\n",
      "Validation Loss: 4.9102, Validation Accuracy: 0.8979\n",
      "0.001\n",
      "Epoch: 97/200\n",
      "Train Loss: 5.3509, Train Accuracy: 0.4559\n",
      "Validation Loss: 4.9079, Validation Accuracy: 0.9006\n",
      "0.001\n",
      "Epoch: 98/200\n",
      "Train Loss: 5.3501, Train Accuracy: 0.4566\n",
      "Validation Loss: 4.9067, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 99/200\n",
      "Train Loss: 5.3608, Train Accuracy: 0.4456\n",
      "Validation Loss: 4.9067, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 100/200\n",
      "Train Loss: 5.3546, Train Accuracy: 0.4515\n",
      "Validation Loss: 4.9098, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 101/200\n",
      "Train Loss: 5.3550, Train Accuracy: 0.4510\n",
      "Validation Loss: 4.9077, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 102/200\n",
      "Train Loss: 5.3554, Train Accuracy: 0.4514\n",
      "Validation Loss: 4.9079, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 103/200\n",
      "Train Loss: 5.3500, Train Accuracy: 0.4567\n",
      "Validation Loss: 4.9081, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 104/200\n",
      "Train Loss: 5.3559, Train Accuracy: 0.4508\n",
      "Validation Loss: 4.9112, Validation Accuracy: 0.8967\n",
      "0.001\n",
      "Epoch: 105/200\n",
      "Train Loss: 5.3534, Train Accuracy: 0.4529\n",
      "Validation Loss: 4.9088, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 106/200\n",
      "Train Loss: 5.3529, Train Accuracy: 0.4536\n",
      "Validation Loss: 4.9080, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 107/200\n",
      "Train Loss: 5.3571, Train Accuracy: 0.4494\n",
      "Validation Loss: 4.9093, Validation Accuracy: 0.8985\n",
      "0.001\n",
      "Epoch: 108/200\n",
      "Train Loss: 5.3545, Train Accuracy: 0.4517\n",
      "Validation Loss: 4.9084, Validation Accuracy: 0.8991\n",
      "0.001\n",
      "Epoch: 109/200\n",
      "Train Loss: 5.3475, Train Accuracy: 0.4589\n",
      "Validation Loss: 4.9092, Validation Accuracy: 0.8991\n",
      "0.001\n",
      "Epoch: 110/200\n",
      "Train Loss: 5.3549, Train Accuracy: 0.4513\n",
      "Validation Loss: 4.9079, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 111/200\n",
      "Train Loss: 5.3530, Train Accuracy: 0.4534\n",
      "Validation Loss: 4.9068, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 112/200\n",
      "Train Loss: 5.3504, Train Accuracy: 0.4556\n",
      "Validation Loss: 4.9096, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 113/200\n",
      "Train Loss: 5.3506, Train Accuracy: 0.4561\n",
      "Validation Loss: 4.9086, Validation Accuracy: 0.8988\n",
      "0.001\n",
      "Epoch: 114/200\n",
      "Train Loss: 5.3477, Train Accuracy: 0.4585\n",
      "Validation Loss: 4.9079, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 115/200\n",
      "Train Loss: 5.3540, Train Accuracy: 0.4524\n",
      "Validation Loss: 4.9086, Validation Accuracy: 0.8991\n",
      "0.001\n",
      "Epoch: 116/200\n",
      "Train Loss: 5.3538, Train Accuracy: 0.4529\n",
      "Validation Loss: 4.9073, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 117/200\n",
      "Train Loss: 5.3559, Train Accuracy: 0.4504\n",
      "Validation Loss: 4.9077, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 118/200\n",
      "Train Loss: 5.3470, Train Accuracy: 0.4593\n",
      "Validation Loss: 4.9078, Validation Accuracy: 0.9003\n",
      "0.001\n",
      "Epoch: 119/200\n",
      "Train Loss: 5.3566, Train Accuracy: 0.4501\n",
      "Validation Loss: 4.9076, Validation Accuracy: 0.9000\n",
      "0.001\n",
      "Epoch: 120/200\n",
      "Train Loss: 5.3527, Train Accuracy: 0.4539\n",
      "Validation Loss: 4.9072, Validation Accuracy: 0.9006\n",
      "0.001\n",
      "Epoch: 121/200\n",
      "Train Loss: 5.3560, Train Accuracy: 0.4501\n",
      "Validation Loss: 4.9080, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 122/200\n",
      "Train Loss: 5.3550, Train Accuracy: 0.4511\n",
      "Validation Loss: 4.9064, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 123/200\n",
      "Train Loss: 5.3550, Train Accuracy: 0.4516\n",
      "Validation Loss: 4.9060, Validation Accuracy: 0.9018\n",
      "0.001\n",
      "Epoch: 124/200\n",
      "Train Loss: 5.3513, Train Accuracy: 0.4552\n",
      "Validation Loss: 4.9055, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 125/200\n",
      "Train Loss: 5.3472, Train Accuracy: 0.4593\n",
      "Validation Loss: 4.9052, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 126/200\n",
      "Train Loss: 5.3517, Train Accuracy: 0.4550\n",
      "Validation Loss: 4.9045, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 127/200\n",
      "Train Loss: 5.3553, Train Accuracy: 0.4510\n",
      "Validation Loss: 4.9058, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 128/200\n",
      "Train Loss: 5.3497, Train Accuracy: 0.4569\n",
      "Validation Loss: 4.9051, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 129/200\n",
      "Train Loss: 5.3468, Train Accuracy: 0.4594\n",
      "Validation Loss: 4.9051, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 130/200\n",
      "Train Loss: 5.3534, Train Accuracy: 0.4523\n",
      "Validation Loss: 4.9040, Validation Accuracy: 0.9039\n",
      "0.001\n",
      "Epoch: 131/200\n",
      "Train Loss: 5.3472, Train Accuracy: 0.4594\n",
      "Validation Loss: 4.9036, Validation Accuracy: 0.9042\n",
      "0.001\n",
      "Epoch: 132/200\n",
      "Train Loss: 5.3644, Train Accuracy: 0.4421\n",
      "Validation Loss: 4.9045, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 133/200\n",
      "Train Loss: 5.3473, Train Accuracy: 0.4589\n",
      "Validation Loss: 4.9054, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 134/200\n",
      "Train Loss: 5.3545, Train Accuracy: 0.4519\n",
      "Validation Loss: 4.9052, Validation Accuracy: 0.9024\n",
      "0.001\n",
      "Epoch: 135/200\n",
      "Train Loss: 5.3473, Train Accuracy: 0.4588\n",
      "Validation Loss: 4.9049, Validation Accuracy: 0.9024\n",
      "0.001\n",
      "Epoch: 136/200\n",
      "Train Loss: 5.3501, Train Accuracy: 0.4563\n",
      "Validation Loss: 4.9050, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 137/200\n",
      "Train Loss: 5.3500, Train Accuracy: 0.4567\n",
      "Validation Loss: 4.9060, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 138/200\n",
      "Train Loss: 5.3510, Train Accuracy: 0.4552\n",
      "Validation Loss: 4.9045, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 139/200\n",
      "Train Loss: 5.3451, Train Accuracy: 0.4616\n",
      "Validation Loss: 4.9051, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 140/200\n",
      "Train Loss: 5.3465, Train Accuracy: 0.4604\n",
      "Validation Loss: 4.9075, Validation Accuracy: 0.8997\n",
      "0.001\n",
      "Epoch: 141/200\n",
      "Train Loss: 5.3538, Train Accuracy: 0.4519\n",
      "Validation Loss: 4.9065, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 142/200\n",
      "Train Loss: 5.3517, Train Accuracy: 0.4545\n",
      "Validation Loss: 4.9065, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 143/200\n",
      "Train Loss: 5.3498, Train Accuracy: 0.4571\n",
      "Validation Loss: 4.9063, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 144/200\n",
      "Train Loss: 5.3563, Train Accuracy: 0.4509\n",
      "Validation Loss: 4.9046, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 145/200\n",
      "Train Loss: 5.3510, Train Accuracy: 0.4561\n",
      "Validation Loss: 4.9065, Validation Accuracy: 0.9018\n",
      "0.001\n",
      "Epoch: 146/200\n",
      "Train Loss: 5.3488, Train Accuracy: 0.4577\n",
      "Validation Loss: 4.9058, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 147/200\n",
      "Train Loss: 5.3484, Train Accuracy: 0.4579\n",
      "Validation Loss: 4.9038, Validation Accuracy: 0.9045\n",
      "0.001\n",
      "Epoch: 148/200\n",
      "Train Loss: 5.3506, Train Accuracy: 0.4558\n",
      "Validation Loss: 4.9047, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 149/200\n",
      "Train Loss: 5.3529, Train Accuracy: 0.4535\n",
      "Validation Loss: 4.9056, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 150/200\n",
      "Train Loss: 5.3524, Train Accuracy: 0.4543\n",
      "Validation Loss: 4.9056, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 151/200\n",
      "Train Loss: 5.3506, Train Accuracy: 0.4562\n",
      "Validation Loss: 4.9041, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 152/200\n",
      "Train Loss: 5.3528, Train Accuracy: 0.4540\n",
      "Validation Loss: 4.9031, Validation Accuracy: 0.9052\n",
      "0.001\n",
      "Epoch: 153/200\n",
      "Train Loss: 5.3520, Train Accuracy: 0.4543\n",
      "Validation Loss: 4.9032, Validation Accuracy: 0.9045\n",
      "0.001\n",
      "Epoch: 154/200\n",
      "Train Loss: 5.3487, Train Accuracy: 0.4582\n",
      "Validation Loss: 4.9033, Validation Accuracy: 0.9045\n",
      "0.001\n",
      "Epoch: 155/200\n",
      "Train Loss: 5.3544, Train Accuracy: 0.4517\n",
      "Validation Loss: 4.9034, Validation Accuracy: 0.9048\n",
      "0.001\n",
      "Epoch: 156/200\n",
      "Train Loss: 5.3501, Train Accuracy: 0.4565\n",
      "Validation Loss: 4.9044, Validation Accuracy: 0.9036\n",
      "0.001\n",
      "Epoch: 157/200\n",
      "Train Loss: 5.3552, Train Accuracy: 0.4518\n",
      "Validation Loss: 4.9048, Validation Accuracy: 0.9036\n",
      "0.001\n",
      "Epoch: 158/200\n",
      "Train Loss: 5.3467, Train Accuracy: 0.4596\n",
      "Validation Loss: 4.9039, Validation Accuracy: 0.9042\n",
      "0.001\n",
      "Epoch: 159/200\n",
      "Train Loss: 5.3479, Train Accuracy: 0.4589\n",
      "Validation Loss: 4.9036, Validation Accuracy: 0.9039\n",
      "0.001\n",
      "Epoch: 160/200\n",
      "Train Loss: 5.3493, Train Accuracy: 0.4573\n",
      "Validation Loss: 4.9034, Validation Accuracy: 0.9042\n",
      "0.001\n",
      "Epoch: 161/200\n",
      "Train Loss: 5.3517, Train Accuracy: 0.4546\n",
      "Validation Loss: 4.9034, Validation Accuracy: 0.9045\n",
      "0.001\n",
      "Epoch: 162/200\n",
      "Train Loss: 5.3502, Train Accuracy: 0.4563\n",
      "Validation Loss: 4.9056, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 163/200\n",
      "Train Loss: 5.3499, Train Accuracy: 0.4563\n",
      "Validation Loss: 4.9045, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 164/200\n",
      "Train Loss: 5.3495, Train Accuracy: 0.4564\n",
      "Validation Loss: 4.9050, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 165/200\n",
      "Train Loss: 5.3536, Train Accuracy: 0.4528\n",
      "Validation Loss: 4.9043, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 166/200\n",
      "Train Loss: 5.3531, Train Accuracy: 0.4535\n",
      "Validation Loss: 4.9059, Validation Accuracy: 0.9018\n",
      "0.001\n",
      "Epoch: 167/200\n",
      "Train Loss: 5.3534, Train Accuracy: 0.4533\n",
      "Validation Loss: 4.9056, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 168/200\n",
      "Train Loss: 5.3498, Train Accuracy: 0.4566\n",
      "Validation Loss: 4.9039, Validation Accuracy: 0.9039\n",
      "0.001\n",
      "Epoch: 169/200\n",
      "Train Loss: 5.3500, Train Accuracy: 0.4568\n",
      "Validation Loss: 4.9055, Validation Accuracy: 0.9021\n",
      "0.001\n",
      "Epoch: 170/200\n",
      "Train Loss: 5.3526, Train Accuracy: 0.4543\n",
      "Validation Loss: 4.9027, Validation Accuracy: 0.9055\n",
      "0.001\n",
      "Epoch: 171/200\n",
      "Train Loss: 5.3482, Train Accuracy: 0.4584\n",
      "Validation Loss: 4.9041, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 172/200\n",
      "Train Loss: 5.3501, Train Accuracy: 0.4564\n",
      "Validation Loss: 4.9053, Validation Accuracy: 0.9027\n",
      "0.001\n",
      "Epoch: 173/200\n",
      "Train Loss: 5.3546, Train Accuracy: 0.4521\n",
      "Validation Loss: 4.9058, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 174/200\n",
      "Train Loss: 5.3520, Train Accuracy: 0.4544\n",
      "Validation Loss: 4.9045, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 175/200\n",
      "Train Loss: 5.3436, Train Accuracy: 0.4627\n",
      "Validation Loss: 4.9059, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 176/200\n",
      "Train Loss: 5.3544, Train Accuracy: 0.4524\n",
      "Validation Loss: 4.9058, Validation Accuracy: 0.9012\n",
      "0.001\n",
      "Epoch: 177/200\n",
      "Train Loss: 5.3515, Train Accuracy: 0.4554\n",
      "Validation Loss: 4.9060, Validation Accuracy: 0.9015\n",
      "0.001\n",
      "Epoch: 178/200\n",
      "Train Loss: 5.3572, Train Accuracy: 0.4490\n",
      "Validation Loss: 4.9043, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 179/200\n",
      "Train Loss: 5.3515, Train Accuracy: 0.4549\n",
      "Validation Loss: 4.9039, Validation Accuracy: 0.9042\n",
      "0.001\n",
      "Epoch: 180/200\n",
      "Train Loss: 5.3495, Train Accuracy: 0.4573\n",
      "Validation Loss: 4.9038, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 181/200\n",
      "Train Loss: 5.3486, Train Accuracy: 0.4584\n",
      "Validation Loss: 4.9044, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 182/200\n",
      "Train Loss: 5.3467, Train Accuracy: 0.4594\n",
      "Validation Loss: 4.9048, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 183/200\n",
      "Train Loss: 5.3397, Train Accuracy: 0.4671\n",
      "Validation Loss: 4.9045, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 184/200\n",
      "Train Loss: 5.3504, Train Accuracy: 0.4563\n",
      "Validation Loss: 4.9035, Validation Accuracy: 0.9039\n",
      "0.001\n",
      "Epoch: 185/200\n",
      "Train Loss: 5.3480, Train Accuracy: 0.4592\n",
      "Validation Loss: 4.9046, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 186/200\n",
      "Train Loss: 5.3538, Train Accuracy: 0.4528\n",
      "Validation Loss: 4.9039, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 187/200\n",
      "Train Loss: 5.3572, Train Accuracy: 0.4492\n",
      "Validation Loss: 4.9034, Validation Accuracy: 0.9042\n",
      "0.001\n",
      "Epoch: 188/200\n",
      "Train Loss: 5.3535, Train Accuracy: 0.4536\n",
      "Validation Loss: 4.9040, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 189/200\n",
      "Train Loss: 5.3500, Train Accuracy: 0.4567\n",
      "Validation Loss: 4.9049, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 190/200\n",
      "Train Loss: 5.3492, Train Accuracy: 0.4572\n",
      "Validation Loss: 4.9039, Validation Accuracy: 0.9039\n",
      "0.001\n",
      "Epoch: 191/200\n",
      "Train Loss: 5.3529, Train Accuracy: 0.4535\n",
      "Validation Loss: 4.9041, Validation Accuracy: 0.9033\n",
      "0.001\n",
      "Epoch: 192/200\n",
      "Train Loss: 5.3468, Train Accuracy: 0.4596\n",
      "Validation Loss: 4.9037, Validation Accuracy: 0.9045\n",
      "0.001\n",
      "Epoch: 193/200\n",
      "Train Loss: 5.3516, Train Accuracy: 0.4554\n",
      "Validation Loss: 4.9036, Validation Accuracy: 0.9039\n",
      "0.001\n",
      "Epoch: 194/200\n",
      "Train Loss: 5.3509, Train Accuracy: 0.4560\n",
      "Validation Loss: 4.9030, Validation Accuracy: 0.9045\n",
      "0.001\n",
      "Epoch: 195/200\n",
      "Train Loss: 5.3493, Train Accuracy: 0.4579\n",
      "Validation Loss: 4.9024, Validation Accuracy: 0.9061\n",
      "0.001\n",
      "Epoch: 196/200\n",
      "Train Loss: 5.3566, Train Accuracy: 0.4497\n",
      "Validation Loss: 4.9068, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 197/200\n",
      "Train Loss: 5.3519, Train Accuracy: 0.4542\n",
      "Validation Loss: 4.9056, Validation Accuracy: 0.9018\n",
      "0.001\n",
      "Epoch: 198/200\n",
      "Train Loss: 5.3488, Train Accuracy: 0.4575\n",
      "Validation Loss: 4.9048, Validation Accuracy: 0.9030\n",
      "0.001\n",
      "Epoch: 199/200\n",
      "Train Loss: 5.3522, Train Accuracy: 0.4543\n",
      "Validation Loss: 4.9067, Validation Accuracy: 0.9009\n",
      "0.001\n",
      "Epoch: 200/200\n",
      "Train Loss: 5.3466, Train Accuracy: 0.4606\n",
      "Validation Loss: 4.9061, Validation Accuracy: 0.9018\n",
      "0.001\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(CustomDataset(X_train, y_train, device), batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(CustomDataset(X_val, y_val, device), batch_size=256)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.functional.cross_entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 100\n",
    "counter = 0\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_loss_accumulator = 0.0\n",
    "    train_correct_accumulator = 0\n",
    "    train_total_samples = 0\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # print(f\"Data: {data}\")\n",
    "        # print(f\"Targets: {targets}\")\n",
    "        # print(f\"Batch: {batch_idx}\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        #print(f\"Outputs: {outputs}\")\n",
    "        #print(f\"Targets: {targets}\")\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_accumulator += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_correct_accumulator += (predicted == targets).sum().item()\n",
    "        train_total_samples += data.size(0)\n",
    "    train_loss = train_loss_accumulator / train_total_samples\n",
    "    train_accuracy = train_correct_accumulator / train_total_samples\n",
    "\n",
    "    #Validation loop\n",
    "    model.eval()\n",
    "    val_loss_accumulator = 0.0\n",
    "    val_correct_accumulator = 0\n",
    "    val_total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for data, targets in val_loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss_accumulator += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct_accumulator += (predicted==targets).sum().item()\n",
    "            val_total_samples += data.size(0)\n",
    "\n",
    "        val_loss = val_loss_accumulator / val_total_samples\n",
    "        val_accuracy = val_correct_accumulator / val_total_samples\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7895ac0d-e1ce-4c34-8906-48b4fde6b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "        299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "        325, 326, 327, 328, 329, 330]),\n",
       " array([70, 66, 70, 82, 31, 70, 19, 70, 68, 26, 73, 68, 68, 33, 63, 73, 36,\n",
       "        58, 62, 64, 68, 70, 73, 39, 64, 76, 73, 46, 84, 61, 45, 66, 48, 82,\n",
       "        63, 37, 82, 69, 67, 69, 67, 68, 44, 67, 57, 67, 35, 33, 64, 75, 69,\n",
       "        76, 66, 46, 99, 67, 64, 67, 56, 57, 66, 68, 52, 78, 67, 60, 68, 68,\n",
       "        45, 25, 73, 49, 42, 64, 69, 44, 70, 70, 51, 64, 67, 66, 67, 74, 66,\n",
       "        78, 73, 73, 59, 53, 70, 54, 73, 74, 64, 37, 34, 66, 67, 52, 58, 64,\n",
       "        79, 69, 52, 68, 70, 36, 57, 69, 55, 21, 69, 76, 60, 69, 36, 66, 49,\n",
       "        75, 64, 67, 76, 76, 66, 66, 73, 70, 62, 66, 69, 64, 33, 42, 70, 45,\n",
       "        64, 54, 64, 69, 64, 65, 72, 66, 96, 64, 69, 34, 66, 55, 73, 49, 51,\n",
       "        54, 69, 46, 65, 69, 66, 74, 60, 64, 22, 57, 73, 37, 35, 69, 37, 71,\n",
       "        34, 66, 67, 27, 66, 66, 69, 69, 93, 28, 76, 66, 42, 69, 67, 33, 44,\n",
       "        54, 69, 57, 48, 78, 61, 70, 64, 64, 78, 66, 67, 38, 71, 55, 75, 64,\n",
       "        66, 29, 72, 81, 69, 15, 39, 67, 27, 76, 73, 75, 24, 71, 67, 75, 35,\n",
       "        82, 70, 67, 65, 43, 37, 69, 75, 68, 35, 73, 64, 58, 67, 69, 58, 64,\n",
       "        69, 61, 71, 67, 52, 72, 35, 77, 78, 56, 80, 68, 38, 73, 81, 30, 34,\n",
       "        24, 25, 61, 78, 58, 70, 63, 64, 28, 45, 73, 77, 49, 67, 64, 60, 63,\n",
       "        63, 70, 73, 69, 25, 71, 28, 59, 67, 60, 43, 72, 66, 67, 75, 61, 59,\n",
       "        67, 70, 48, 34, 66, 70, 46, 69, 82, 45, 76, 61, 52, 69, 93, 38, 67,\n",
       "        79, 67, 67, 72, 67, 69, 77, 66, 99, 66, 66, 73, 65, 79, 72, 45, 33,\n",
       "        34, 43, 69, 79, 73, 23, 46, 70]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# does my validation set contain the same speakers as my training set?\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2cc2f937-63bf-45e6-96f1-812c969c0534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "        299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "        325, 326, 327, 328, 329, 330]),\n",
       " array([11, 11, 12, 14,  5, 11,  3, 12, 11,  4, 12, 11, 11,  5, 10, 12,  6,\n",
       "         9, 10, 11, 11, 12, 12,  6, 11, 12, 12,  8, 14, 10,  7, 11,  8, 13,\n",
       "        10,  6, 14, 11, 11, 11, 11, 11,  7, 11,  9, 11,  6,  5, 11, 12, 11,\n",
       "        12, 11,  7, 16, 11, 10, 11,  9,  9, 11, 11,  9, 13, 11, 10, 11, 11,\n",
       "         7,  4, 12,  8,  7, 10, 11,  7, 12, 12,  8, 11, 11, 11, 11, 12, 11,\n",
       "        13, 12, 12, 10,  9, 12,  9, 12, 12, 11,  6,  6, 11, 11,  8,  9, 10,\n",
       "        13, 11,  8, 11, 11,  6,  9, 11,  9,  3, 11, 13, 10, 11,  6, 11,  8,\n",
       "        12, 11, 11, 13, 13, 11, 11, 12, 11, 10, 11, 11, 10,  5,  7, 12,  7,\n",
       "        10,  9, 10, 11, 11, 11, 12, 11, 16, 10, 11,  5, 11,  9, 12,  8,  8,\n",
       "         9, 11,  8, 11, 11, 11, 12, 10, 10,  4,  9, 12,  6,  6, 11,  6, 12,\n",
       "         5, 11, 11,  4, 11, 11, 11, 11, 15,  4, 12, 11,  7, 11, 11,  5,  7,\n",
       "         9, 11,  9,  8, 13, 10, 12, 11, 11, 13, 11, 11,  6, 12,  9, 12, 11,\n",
       "        11,  5, 12, 13, 11,  2,  6, 11,  4, 12, 12, 12,  4, 12, 11, 12,  6,\n",
       "        13, 11, 11, 11,  7,  6, 11, 12, 11,  6, 12, 10, 10, 11, 11, 10, 11,\n",
       "        11, 10, 12, 11,  8, 12,  6, 13, 13,  9, 13, 11,  6, 12, 13,  5,  5,\n",
       "         4,  4, 10, 13, 10, 12, 10, 11,  4,  7, 12, 13,  8, 11, 10, 10, 10,\n",
       "        10, 11, 12, 11,  4, 12,  4, 10, 11, 10,  7, 12, 11, 11, 12, 10, 10,\n",
       "        11, 12,  8,  5, 11, 11,  8, 11, 13,  7, 13, 10,  9, 11, 15,  6, 11,\n",
       "        13, 11, 11, 12, 11, 11, 13, 11, 16, 11, 11, 12, 11, 13, 12,  7,  5,\n",
       "         6,  7, 11, 13, 12,  4,  7, 12]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_val, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fdcf2559-0215-4a6a-a936-80db190af23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f38cf7ac-f2d3-4b72-831a-29800f8d273e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 228 - 228\n",
      "1: 228 - 228\n",
      "2: 228 - 228\n",
      "3: 228 - 4\n",
      "4: 228 - 1\n",
      "5: 228 - 1\n",
      "6: 228 - 321\n",
      "7: 228 - 3\n",
      "8: 228 - 228\n",
      "9: 228 - 228\n",
      "10: 228 - 0\n",
      "11: 228 - 228\n",
      "12: 228 - 0\n",
      "13: 228 - 250\n",
      "14: 228 - 319\n",
      "15: 228 - 228\n",
      "16: 228 - 319\n",
      "17: 228 - 228\n",
      "18: 228 - 228\n",
      "19: 228 - 228\n",
      "20: 228 - 228\n",
      "21: 228 - 228\n",
      "22: 228 - 228\n",
      "23: 228 - 39\n",
      "24: 228 - 228\n",
      "25: 228 - 228\n",
      "26: 228 - 2\n",
      "27: 228 - 228\n",
      "28: 228 - 0\n",
      "29: 228 - 228\n",
      "30: 228 - 228\n",
      "31: 228 - 228\n",
      "32: 228 - 321\n",
      "33: 228 - 228\n",
      "34: 228 - 228\n",
      "35: 228 - 2\n",
      "36: 228 - 23\n",
      "37: 228 - 0\n",
      "38: 228 - 2\n",
      "39: 228 - 228\n",
      "40: 228 - 0\n",
      "41: 228 - 321\n",
      "42: 228 - 228\n",
      "43: 228 - 37\n",
      "44: 228 - 228\n",
      "45: 228 - 250\n",
      "46: 228 - 228\n",
      "47: 228 - 321\n",
      "48: 228 - 228\n",
      "49: 228 - 250\n",
      "50: 228 - 275\n",
      "51: 228 - 228\n",
      "52: 228 - 228\n",
      "53: 228 - 250\n",
      "54: 228 - 0\n",
      "55: 228 - 321\n",
      "56: 228 - 228\n",
      "57: 228 - 228\n",
      "58: 228 - 250\n",
      "59: 228 - 39\n",
      "60: 228 - 228\n",
      "61: 228 - 228\n",
      "62: 228 - 228\n",
      "63: 228 - 228\n",
      "64: 228 - 60\n",
      "65: 228 - 0\n",
      "66: 228 - 4\n",
      "67: 228 - 228\n",
      "68: 228 - 1\n",
      "69: 228 - 228\n",
      "70: 228 - 60\n",
      "71: 228 - 228\n",
      "72: 228 - 228\n",
      "73: 228 - 321\n",
      "74: 228 - 228\n",
      "75: 78 - 0\n",
      "76: 78 - 0\n",
      "77: 78 - 1\n",
      "78: 78 - 78\n",
      "79: 78 - 78\n",
      "80: 78 - 78\n",
      "81: 78 - 260\n",
      "82: 78 - 2\n",
      "83: 78 - 2\n",
      "84: 78 - 78\n",
      "85: 78 - 0\n",
      "86: 78 - 78\n",
      "87: 78 - 192\n",
      "88: 78 - 1\n",
      "89: 78 - 78\n",
      "90: 78 - 78\n",
      "91: 78 - 78\n",
      "92: 78 - 78\n",
      "93: 78 - 78\n",
      "94: 78 - 78\n",
      "95: 78 - 142\n",
      "96: 78 - 78\n",
      "97: 78 - 202\n",
      "98: 78 - 202\n",
      "99: 78 - 78\n",
      "100: 78 - 78\n",
      "101: 78 - 78\n",
      "102: 78 - 78\n",
      "103: 78 - 78\n",
      "104: 78 - 78\n",
      "105: 78 - 78\n",
      "106: 78 - 78\n",
      "107: 78 - 97\n",
      "108: 78 - 78\n",
      "109: 78 - 78\n",
      "110: 78 - 155\n",
      "111: 78 - 3\n",
      "112: 78 - 142\n",
      "113: 78 - 78\n",
      "114: 78 - 78\n",
      "115: 78 - 155\n",
      "116: 78 - 2\n",
      "117: 78 - 78\n",
      "118: 78 - 78\n",
      "119: 78 - 0\n",
      "120: 78 - 78\n",
      "121: 78 - 78\n",
      "122: 78 - 3\n",
      "123: 78 - 78\n",
      "124: 78 - 1\n",
      "125: 78 - 192\n",
      "126: 284 - 0\n",
      "127: 284 - 0\n",
      "128: 284 - 220\n",
      "129: 284 - 0\n",
      "130: 284 - 284\n",
      "131: 284 - 284\n",
      "132: 284 - 76\n",
      "133: 284 - 220\n",
      "134: 284 - 264\n",
      "135: 284 - 284\n",
      "136: 284 - 2\n",
      "137: 284 - 284\n",
      "138: 284 - 307\n",
      "139: 284 - 90\n",
      "140: 284 - 2\n",
      "141: 284 - 284\n",
      "142: 284 - 284\n",
      "143: 284 - 284\n",
      "144: 284 - 284\n",
      "145: 284 - 284\n",
      "146: 284 - 71\n",
      "147: 284 - 0\n",
      "148: 284 - 284\n",
      "149: 284 - 284\n",
      "150: 284 - 1\n",
      "151: 284 - 1\n",
      "152: 284 - 284\n",
      "153: 284 - 284\n",
      "154: 284 - 0\n",
      "155: 284 - 1\n",
      "156: 284 - 284\n",
      "157: 284 - 307\n",
      "158: 284 - 284\n",
      "159: 284 - 1\n",
      "160: 284 - 0\n",
      "161: 284 - 0\n",
      "162: 284 - 284\n",
      "163: 284 - 284\n",
      "164: 284 - 284\n",
      "165: 284 - 284\n",
      "166: 284 - 284\n",
      "167: 284 - 284\n",
      "168: 284 - 95\n",
      "169: 284 - 95\n",
      "170: 284 - 95\n",
      "171: 284 - 193\n",
      "172: 284 - 95\n",
      "173: 284 - 284\n",
      "174: 284 - 1\n",
      "175: 284 - 284\n",
      "176: 284 - 0\n",
      "177: 284 - 307\n",
      "178: 284 - 284\n",
      "179: 284 - 284\n",
      "180: 284 - 0\n",
      "181: 284 - 1\n",
      "182: 284 - 284\n",
      "183: 284 - 284\n",
      "184: 284 - 284\n",
      "185: 284 - 284\n",
      "186: 284 - 264\n",
      "187: 284 - 284\n",
      "188: 284 - 0\n",
      "189: 284 - 0\n",
      "190: 284 - 284\n",
      "191: 284 - 0\n",
      "192: 84 - 0\n",
      "193: 84 - 1\n",
      "194: 84 - 84\n",
      "195: 84 - 84\n",
      "196: 84 - 84\n",
      "197: 84 - 108\n",
      "198: 84 - 84\n",
      "199: 84 - 84\n",
      "200: 84 - 59\n",
      "201: 84 - 84\n",
      "202: 84 - 84\n",
      "203: 84 - 0\n",
      "204: 84 - 84\n",
      "205: 84 - 0\n",
      "206: 84 - 84\n",
      "207: 84 - 74\n",
      "208: 84 - 2\n",
      "209: 84 - 84\n",
      "210: 84 - 102\n",
      "211: 84 - 84\n",
      "212: 84 - 2\n",
      "213: 84 - 214\n",
      "214: 84 - 84\n",
      "215: 84 - 128\n",
      "216: 84 - 84\n",
      "217: 84 - 214\n",
      "218: 84 - 128\n",
      "219: 84 - 288\n",
      "220: 84 - 294\n",
      "221: 84 - 74\n",
      "222: 84 - 3\n",
      "223: 84 - 215\n",
      "224: 84 - 84\n",
      "225: 84 - 0\n",
      "226: 84 - 0\n",
      "227: 84 - 84\n",
      "228: 84 - 214\n",
      "229: 84 - 84\n",
      "230: 84 - 84\n",
      "231: 84 - 294\n",
      "232: 84 - 71\n",
      "233: 84 - 214\n",
      "234: 84 - 71\n",
      "235: 84 - 84\n",
      "236: 84 - 84\n",
      "237: 84 - 84\n",
      "238: 84 - 215\n",
      "239: 84 - 84\n",
      "240: 84 - 84\n",
      "241: 84 - 84\n",
      "242: 84 - 71\n",
      "243: 84 - 84\n",
      "244: 84 - 58\n",
      "245: 84 - 84\n",
      "246: 84 - 84\n",
      "247: 84 - 84\n",
      "248: 84 - 84\n",
      "249: 84 - 84\n",
      "250: 84 - 215\n",
      "251: 84 - 71\n",
      "252: 84 - 0\n",
      "253: 84 - 21\n",
      "254: 84 - 215\n",
      "255: 84 - 71\n",
      "256: 84 - 0\n",
      "257: 84 - 33\n",
      "258: 273 - 4\n",
      "259: 273 - 273\n",
      "260: 273 - 273\n",
      "261: 273 - 273\n",
      "262: 273 - 273\n",
      "263: 273 - 166\n",
      "264: 273 - 273\n",
      "265: 273 - 273\n",
      "266: 273 - 273\n",
      "267: 273 - 273\n",
      "268: 273 - 0\n",
      "269: 273 - 139\n",
      "270: 273 - 273\n",
      "271: 273 - 5\n",
      "272: 273 - 273\n",
      "273: 273 - 0\n",
      "274: 273 - 18\n",
      "275: 273 - 273\n",
      "276: 273 - 0\n",
      "277: 273 - 273\n",
      "278: 273 - 273\n",
      "279: 273 - 273\n",
      "280: 273 - 1\n",
      "281: 273 - 21\n",
      "282: 273 - 1\n",
      "283: 273 - 2\n",
      "284: 273 - 311\n",
      "285: 273 - 1\n",
      "286: 273 - 2\n",
      "287: 273 - 273\n",
      "288: 273 - 273\n",
      "289: 273 - 273\n",
      "290: 273 - 273\n",
      "291: 273 - 273\n",
      "292: 273 - 4\n",
      "293: 273 - 273\n",
      "294: 273 - 104\n",
      "295: 273 - 2\n",
      "296: 273 - 273\n",
      "297: 273 - 104\n",
      "298: 273 - 273\n",
      "299: 273 - 281\n",
      "300: 273 - 1\n",
      "301: 273 - 273\n",
      "302: 273 - 273\n",
      "303: 273 - 0\n",
      "304: 273 - 0\n",
      "305: 273 - 273\n",
      "306: 273 - 273\n",
      "307: 273 - 273\n",
      "308: 273 - 0\n",
      "309: 273 - 0\n",
      "310: 273 - 273\n",
      "311: 273 - 3\n",
      "312: 273 - 1\n",
      "313: 273 - 3\n",
      "314: 273 - 273\n",
      "315: 273 - 0\n",
      "316: 273 - 230\n",
      "317: 273 - 273\n",
      "318: 273 - 1\n",
      "319: 273 - 273\n",
      "320: 273 - 273\n",
      "321: 273 - 273\n",
      "322: 273 - 0\n",
      "323: 273 - 273\n",
      "324: 273 - 196\n",
      "325: 273 - 273\n",
      "326: 273 - 273\n",
      "327: 273 - 273\n",
      "328: 316 - 316\n",
      "329: 316 - 0\n",
      "330: 316 - 316\n",
      "331: 316 - 316\n",
      "332: 316 - 0\n",
      "333: 316 - 0\n",
      "334: 316 - 71\n",
      "335: 316 - 316\n",
      "336: 316 - 316\n",
      "337: 316 - 1\n",
      "338: 316 - 316\n",
      "339: 316 - 3\n",
      "340: 316 - 316\n",
      "341: 316 - 213\n",
      "342: 316 - 316\n",
      "343: 316 - 71\n",
      "344: 316 - 1\n",
      "345: 316 - 70\n",
      "346: 316 - 71\n",
      "347: 316 - 1\n",
      "348: 316 - 316\n",
      "349: 316 - 70\n",
      "350: 316 - 320\n",
      "351: 316 - 316\n",
      "352: 316 - 161\n",
      "353: 316 - 316\n",
      "354: 316 - 1\n",
      "355: 316 - 316\n",
      "356: 316 - 316\n",
      "357: 316 - 161\n",
      "358: 316 - 6\n",
      "359: 316 - 316\n",
      "360: 316 - 71\n",
      "361: 316 - 316\n",
      "362: 316 - 316\n",
      "363: 316 - 316\n",
      "364: 316 - 316\n",
      "365: 316 - 320\n",
      "366: 316 - 316\n",
      "367: 316 - 161\n",
      "368: 316 - 161\n",
      "369: 316 - 316\n",
      "370: 316 - 316\n",
      "371: 316 - 316\n",
      "372: 316 - 0\n",
      "373: 316 - 316\n",
      "374: 316 - 0\n",
      "375: 316 - 71\n",
      "376: 316 - 316\n",
      "377: 316 - 0\n",
      "378: 316 - 316\n",
      "379: 316 - 316\n",
      "380: 316 - 71\n",
      "381: 316 - 71\n",
      "382: 316 - 316\n",
      "383: 316 - 316\n",
      "384: 316 - 316\n",
      "385: 316 - 316\n",
      "386: 316 - 71\n",
      "387: 316 - 316\n",
      "388: 316 - 316\n",
      "389: 316 - 316\n",
      "390: 316 - 71\n",
      "391: 316 - 0\n",
      "392: 316 - 316\n",
      "393: 316 - 316\n",
      "394: 80 - 80\n",
      "395: 80 - 80\n",
      "396: 80 - 80\n",
      "397: 80 - 80\n",
      "398: 80 - 80\n",
      "399: 80 - 0\n",
      "400: 80 - 2\n",
      "401: 80 - 215\n",
      "402: 80 - 80\n",
      "403: 80 - 2\n",
      "404: 80 - 7\n",
      "405: 80 - 80\n",
      "406: 80 - 312\n",
      "407: 80 - 215\n",
      "408: 80 - 0\n",
      "409: 80 - 2\n",
      "410: 80 - 80\n",
      "411: 80 - 80\n",
      "412: 80 - 261\n",
      "413: 80 - 80\n",
      "414: 80 - 80\n",
      "415: 80 - 80\n",
      "416: 80 - 80\n",
      "417: 80 - 261\n",
      "418: 80 - 80\n",
      "419: 80 - 80\n",
      "420: 80 - 0\n",
      "421: 80 - 80\n",
      "422: 80 - 35\n",
      "423: 80 - 215\n",
      "424: 80 - 80\n",
      "425: 80 - 80\n",
      "426: 80 - 1\n",
      "427: 80 - 80\n",
      "428: 80 - 0\n",
      "429: 80 - 80\n",
      "430: 80 - 80\n",
      "431: 80 - 80\n",
      "432: 80 - 315\n",
      "433: 80 - 2\n",
      "434: 80 - 80\n",
      "435: 80 - 215\n",
      "436: 80 - 215\n",
      "437: 80 - 7\n",
      "438: 80 - 80\n",
      "439: 80 - 80\n",
      "440: 80 - 80\n",
      "441: 80 - 257\n",
      "442: 80 - 312\n",
      "443: 80 - 36\n",
      "444: 80 - 2\n",
      "445: 80 - 0\n",
      "446: 80 - 80\n",
      "447: 80 - 80\n",
      "448: 80 - 261\n",
      "449: 80 - 306\n",
      "450: 80 - 80\n",
      "451: 80 - 4\n",
      "452: 80 - 80\n",
      "453: 80 - 2\n",
      "454: 80 - 85\n",
      "455: 80 - 80\n",
      "456: 80 - 80\n",
      "457: 80 - 80\n",
      "458: 80 - 80\n",
      "459: 80 - 66\n",
      "460: 80 - 80\n",
      "461: 306 - 174\n",
      "462: 306 - 306\n",
      "463: 306 - 326\n",
      "464: 306 - 306\n",
      "465: 306 - 306\n",
      "466: 306 - 306\n",
      "467: 306 - 306\n",
      "468: 306 - 312\n",
      "469: 306 - 306\n",
      "470: 306 - 1\n",
      "471: 306 - 261\n",
      "472: 306 - 306\n",
      "473: 306 - 306\n",
      "474: 306 - 306\n",
      "475: 306 - 0\n",
      "476: 306 - 306\n",
      "477: 306 - 306\n",
      "478: 306 - 326\n",
      "479: 306 - 1\n",
      "480: 306 - 306\n",
      "481: 306 - 306\n",
      "482: 306 - 306\n",
      "483: 306 - 306\n",
      "484: 306 - 8\n",
      "485: 306 - 0\n",
      "486: 306 - 0\n",
      "487: 306 - 306\n",
      "488: 306 - 319\n",
      "489: 306 - 330\n",
      "490: 306 - 306\n",
      "491: 306 - 89\n",
      "492: 306 - 1\n",
      "493: 306 - 306\n",
      "494: 306 - 306\n",
      "495: 306 - 306\n",
      "496: 306 - 306\n",
      "497: 306 - 245\n",
      "498: 306 - 306\n",
      "499: 306 - 306\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    print(f\"{i}: {y_train[i]} - {model(torch.tensor([X_train[i]], dtype=torch.float32)).argmax()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3dc202-7bfe-4503-a5a0-785975e1221b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
